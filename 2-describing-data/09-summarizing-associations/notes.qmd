---
title: "Summarizing bivariate numerical associations"
subtitle: "Covariance, Pearson correlation, and a taste of non-linear correlation measures"
date: "09/20/2022"
image: images/penguin-plot.png
format:
  html:
    code-fold: true
    code-link: true
    code-summary: "."
execute: 
  warning: false
  message: false
---

Here we are going to discuss ways of quantifying associations between two variables. In other words we are studying descriptive statistics for *bivariate associations*.

Looking at the two plots below we can see

-   There seems to be little to no association between x and y in the first plot

-   There is a positive association between x any y in the second plot

But how would we quantify these statements?

```{r}

library(MASS)
library(tidyverse)
library(cowplot)

# set the random seed so the plot always look the same
set.seed(1) 

n_samples = 500
Sigma_ident = matrix(c(1,0,0,1), 2, 2)
Sigma_is_corr = matrix(c(1,.8,.8, 1), 2, 2)


plot_indep <- mvrnorm(n=n_samples, mu=c(0, 0), Sigma=Sigma_ident) %>% 
    as_tibble() %>% 
    setNames(c('x', 'y')) %>% 
    ggplot(aes(x=x, y=y)) + 
    geom_point() + 
    lims(x=c(-4, 4), y=c(-4, 4)) 


plot_is_corr <- mvrnorm(n=n_samples, mu=c(0, 0), Sigma=Sigma_is_corr) %>% 
    as_tibble() %>% 
    setNames(c('x', 'y')) %>% 
    ggplot(aes(x=x, y=y)) +
    geom_point() +
    lims(x=c(-4, 4), y=c(-4, 4))


plot_grid(plot_indep, plot_is_corr, labels=c("A", "B"), ncol = 2, nrow = 1)

```

# A first measure of bivariate linear association: covariance

If you were asked for a mathematical formula for the "typical value" of a variable you could probably come up with the mean or median. Similarly, if you were asked for a formula for the "typical distance to the typical value" you could probably derive the variance or [median absolute deviation](https://en.wikipedia.org/wiki/Median_absolute_deviation). The covariance formula is not quite so immediately discoverable.

Instead of trying to quantify any possible association between two variables let's restrict our attention to *linear associations* for now. In other words, if our first variable increases, the second variable increases proportionally. What would we want out of a bivariate association statistic?

::: {.callout-warning appearance="simple" icon="false"}
## Desiderata

-   Large positive when there is a strong increasing association
-   Large negative when there is a strong decreasing association
-   Close to 0 when there is no association
:::

### Digression: residuals

Suppose we observe $n$ observations of a variable $x_1, \dots, x_n$. The residuals quantify how far each observation is from the typical value. In detail, the residuals from the mean are $x_1 - \overline{x}$, $x_2 - \overline{x}$, ..., $x_n - \overline{x}$ where we [recall](https://www.stat20.org/2-describing-data/05-summarizing-numerical-data/notes.html#constructing-numerical-summaries) $\overline{x}$ is the mean.

### Covariance formula

To make things concrete suppose we observe $n$ observations of two variables $x_1, \dots, x_n$ and $y_1, \dots, y_n$ and we want to quantify the association between x and y. The covariance formula is

$$
\text{cov(x, y)} = \frac{(x_1 - \overline{x})(y_1 - \overline{y}) + (x_2 - \overline{x})(y_2 - \overline{y}) + \dots + (x_n - \overline{x})(y_n - \overline{y})}{n- 1}
$$
:::{.column-margin}
The covariance formula has an `n-1` instead of an `n` in the denominator for technical reasons not worth getting into at this point. When `n` is large it doesn't really make a difference which one you use. 
:::

In other words the covariance is the "the typical product of x and y residuals".


Doing this in R

```{r}
#| code-fold: false
x <- c(1, 2, 3, 4, 5, 6, 7)
y <- c(2, 4, 6, 8, 10, 12, 15)
cov(x, y)
```

### Special cases to build intuition

It may not be immediately clear how this covariance formula satisfies our desiderata. When we want to understand a complex mathematical object it is often helpful to consider *special cases*.

Special case 1: $x_1 = 0, y_1 = 0, x_2 = 1, y_2 = 1$


```{r}
data.frame(c(0, 1), c(0, 1)) %>% 
    setNames(c('x', 'y')) %>%  
    ggplot(aes(x=x, y=y)) +
    geom_point() + 
    lims(x=c(0, 1), y=c(-1, 1))

```

Here $\overline{x} = \frac{1}{2}$ and $\overline{y} = \frac{1}{2}$ and plugging into the covariance formula

$$
\text{cov} = \frac{(0 - \frac{1}{2})(0 - \frac{1}{2}) + (1 - \frac{1}{2})(1 - \frac{1}{2})}{2} = \frac{\frac{1}{4} + \frac{1}{4}}{2 - 1} = 0.5
$$
i.e. the covariance is positive.

Special case 2: $x_1 = 0, y_1 = 0, x_2 = 1, y_2 = -1$


```{r}
data.frame(c(0, 1), c(0, -1)) %>% 
    setNames(c('x', 'y')) %>%  
    ggplot(aes(x=x, y=y)) +
    geom_point() + 
    lims(x=c(0, 1), y=c(-1, 1))

```

Here $\overline{x} = \frac{1}{2}$ and $\overline{y} = -\frac{1}{2}$ and plugging into the covariance formula

$$
\text{cov} = \frac{(0 - \frac{1}{2})(0 - -\frac{1}{2}) + (1 - \frac{1}{2})(-1 - -\frac{1}{2})}{2} = \frac{-\frac{1}{4} + -\frac{1}{4}}{2 - 1} = -0.5
$$

i.e. the covariance is negative.


Some takeaways from these calculations.

-   If our data points are typically  linearly increasing then the terms in the covariance formula will be mostly positive and the covariance will be a large positive value!

-   If the points are typically linearly decreasing then the covariance will be a large negative value!

-   If each pair of x/y terms are randomly increasing or decreasing then about half the terms in the formula will be positive and half will be negative; here the covariance will be close to 0!


### Is a child's height more associated with their mother or father's height

Let's load a survey of childrens' heights and their parents heights. Note height here is measured in inches.

```{r}
# read in a csv file from a webpage
url <- 'https://raw.githubusercontent.com/data-8/textbook/main/assets/data/family_heights.csv'
family_heights <- read_csv(url)
family_heights
```

Now we can visually inspect

```{r}
family_heights %>% 
    ggplot(aes(x=father, y=childHeight)) + 
    geom_point() + 
    xlab("Father height (inches)") + 
    ylab("Child height (inches)") 

family_heights %>% 
    ggplot(aes(x=mother, y=childHeight)) + 
    geom_point() + 
    xlab("Mother height (inches)") + 
    ylab("Child height (inches)") 
```

It's not exactly clear which association is stronger. Let's compare covariances

```{r}
#| code-fold: false
family_heights %>% 
    summarise(father_cov = cov(father, childHeight),
              mother_cov = cov(mother, childHeight))
```


# Pearson correlation: a rescaled version of covariance

There is something unsatisfactory about covariance; it depends on the scale of the variables. Suppose we had measured mothers' heights in femtometers (femtometer = $10^{-15}$ meters).

```{r}
#| code-fold: false
# 2.54 cm/in, 10^13 femtometers/cm
family_heights <- family_heights %>% 
        mutate(mother_femtometers=13 * mother * 2.54 * 10^13)


family_heights %>% 
    summarise(father_cov = cov(father, childHeight),
              mother_cov = cov(mother_femtometers, childHeight))
```

Suddenly the covariance between child height (inches) and mother height (femtometers) is ridiculously large! The issue here is that covariance depends on the scale of the variables. For example, switching from inches to femtometers multiplies the mother's height variable by $2.54 \cdot 10^{13}$. But changing the unit of measurement should not change the strength of association!

::: {.callout-warning appearance="simple" icon="false"}
## Additional desiderata for linear bivariate association

-   Scale free; multiplying a variable by a (positive) number does not change the association.
:::

### A digression: scaled residuals

One data intrinsic measure of scale is the standard deviation (the square root of the variance). We can make our data residuals scale free by dividing by the standard deviation i.e.$\frac{x_1 - \overline{x}}{\text{sd}(x)}, \frac{x_2 - \overline{x}}{\text{sd}(x)}, \dots$. These residuals now measure the distance from each observation to the typical observation *relative to the natural variability of the dataset.*

One major advantage of the scaled residuals is that multiplying the observations by any positive number does not change the relative residuals. The relative residuals are scale free!

<!-- Perhaps ask why we want sqrt variance not variance i.e. the units-->

### Pearson correlation formula

The correlation is simply the covariance using the scaled residuals!

$$
\text{cor}(x, y) = \frac{\frac{(x_1 - \overline{x})}{sd(x)} \frac{(y_1 - \overline{y})}{sd(y)}  + 
\frac{(x_2 - \overline{x})}{sd(x)} \frac{(y_2 - \overline{y})}{sd(y)} + \dots}{n} = \frac{\text{cov(x, y)}}{\text{sd}(x) \text{sd}(y)}
$$

It is a mathematical fact that the pearson correlation is always between -1 and 1. Moreover it does not depend on the units/scale of the x or y data!

Now let's compare the association between the child height and parent height using correlation. 
```{r}
#| code-fold: false
family_heights %>% 
    summarise(father_cor = cor(father, childHeight),
              mother_cor = cor(mother, childHeight),
              mother_cor_femto = cor(mother_femtometers, childHeight))
```

Notice the change of units does not affect the correlation! 


# Moving beyond linearity

Not all associations are linear. Each of the plots below shows to variables that are associated. The first column shows a moderate association; the second column shows a stronger association; the third column shows a perfect association (we call this a *deterministic relationship*).

```{r}

# Setup data
n_samples = 200

x_vals <- runif(n=n_samples, min=-1, max=1)


y_lin_low <- x_vals + rnorm(n=n_samples, mean=0, sd=1)
y_lin_med <- x_vals + rnorm(n=n_samples, mean=0, sd=.25)
y_lin_det <- x_vals

deg <- 3
y_mono_low <- x_vals^deg + rnorm(n=n_samples, mean=0, sd=1)
y_mono_med <- x_vals^deg + rnorm(n=n_samples, mean=0, sd=.1)
y_mono_det <- x_vals^deg


y_nonlin_low <- cos(x_vals * (2 * pi )) + rnorm(n=n_samples, mean=0, sd=1)
y_nonlin_med <- cos(x_vals * (2 * pi )) + rnorm(n=n_samples, mean=0, sd=.25)
y_nonlin_det <- cos(x_vals * (2 * pi ))

df <- tibble(x_vals, 
             y_lin_low, y_lin_med, y_lin_det,
             y_mono_low, y_mono_med, y_mono_det,
             y_nonlin_low, y_nonlin_med, y_nonlin_det)
```

```{r}
# compute correlations
library(XICOR)


x_vals_vec <- df[, 1][[1]]

corr_df <- tibble(method = character(),
                  variable = character(),
                  corr = numeric())
for (method in c('pearson', 'spearman', 'chatterjee')){
    for (j in 2:dim(df)[2]){
        col <- colnames(df)[j]
        
        y_vals_vec <- df[, j][[1]]
        
        if (method == 'chatterjee'){
            corr_val <- calculateXI(x=x_vals_vec, y=y_vals_vec)
        }else{
            corr_val <- cor(x_vals_vec, y_vals_vec, method=method)
        }
        
        corr_df <- corr_df %>% add_row(method=method, variable=col, corr=corr_val)
    }
}

```

```{r}
title_txt_size <- 5
get_title <- function(corr_df, v){
    
    pc <- corr_df %>% filter(method=='pearson', variable==v) %>% .[[3]]
    sc <- corr_df %>% filter(method=='spearman', variable==v) %>% .[[3]]
    cc <- corr_df %>% filter(method=='chatterjee', variable==v) %>% .[[3]]
    
    
    txt <- ''
    txt <- paste0(txt, 'pearson = ', round(pc, 2))
    txt <- paste0(txt, ', spearman = ', round(sc, 2))
    txt <- paste0(txt, ', chatterjee = ', round(cc, 2))
    txt
}

plot_lin_low <- df %>%
    ggplot(aes(x=x_vals, y=y_lin_low)) +
    geom_point() +
    ggtitle(get_title(corr_df=corr_df, v='y_lin_low'))  + 
    theme(plot.title = element_text(size = title_txt_size)) + 
    labs(x='x', y='y')


plot_lin_med <- df %>% 
    ggplot(aes(x=x_vals, y=y_lin_med)) +
    geom_point() + 
    ggtitle(get_title(corr_df=corr_df, v='y_lin_med'))  + 
    theme(plot.title = element_text(size = title_txt_size)) + 
    labs(x='x', y='y')

plot_lin_det <- df %>% 
    ggplot(aes(x=x_vals, y=y_lin_det)) + 
    geom_point() + 
    ggtitle(get_title(corr_df=corr_df, v='y_lin_det'))  + 
    theme(plot.title = element_text(size = title_txt_size)) + 
    labs(x='x', y='y')

plot_mono_low <- df %>% 
    ggplot(aes(x=x_vals, y=y_mono_low)) + 
    geom_point() + 
    ggtitle(get_title(corr_df=corr_df, v='y_mono_low'))  + 
    theme(plot.title = element_text(size = title_txt_size)) + 
    labs(x='x', y='y')

plot_mono_med <- df %>%
    ggplot(aes(x=x_vals, y=y_mono_med)) + 
    geom_point() + 
    ggtitle(get_title(corr_df=corr_df, v='y_mono_med'))  + 
    theme(plot.title = element_text(size = title_txt_size)) + 
    labs(x='x', y='y')

plot_mono_det <- df %>%
    ggplot(aes(x=x_vals, y=y_mono_det)) + 
    geom_point() + 
    ggtitle(get_title(corr_df=corr_df, v='y_mono_det'))  + 
    theme(plot.title = element_text(size = title_txt_size)) + 
    labs(x='x', y='y')

plot_nonlin_low <- df %>%
    ggplot(aes(x=x_vals, y=y_nonlin_low)) +
    geom_point() + 
    ggtitle(get_title(corr_df=corr_df, v='y_nonlin_low'))  + 
    theme(plot.title = element_text(size = title_txt_size)) + 
    labs(x='x', y='y')

plot_nonlin_med <- df %>%
    ggplot(aes(x=x_vals, y=y_nonlin_med)) + 
    geom_point() + 
    ggtitle(get_title(corr_df=corr_df, v='y_nonlin_med'))  + 
    theme(plot.title = element_text(size = title_txt_size)) + 
    labs(x='x', y='y')

plot_nonlin_det <- df %>%
    ggplot(aes(x=x_vals, y=y_nonlin_det)) +
    geom_point() + 
    ggtitle(get_title(corr_df=corr_df, v='y_nonlin_det'))  + 
    theme(plot.title = element_text(size = title_txt_size)) + 
    labs(x='x', y='y')



plot_grid(plot_lin_low, plot_lin_med, plot_lin_det,
          plot_mono_low, plot_mono_med, plot_mono_det,
          plot_nonlin_low, plot_nonlin_med, plot_nonlin_det,
          ncol = 3, nrow = 3)
```

The first row shows a linear association. The second row shows a non-linear but *monotonic* association. Monotonic positive relation just means that if x goes up then y has to go up -- but it does not need to go up proportionally to the increase in x! A monotonic negative association just means the opposite (x goes up, y goes down). Monotonic is a generalization of linear (linear is monotonic, but montonic is not necessarily linear). We can quantify a monotonic relationship with [Spearman's rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient).

The third row shows a non-linear association that non-monotonic. Quantifying such general associations is not easy. In fact [statisticians are still inventing](https://arxiv.org/abs/1909.10140) ways of quantifying general relationships such as Chatterjee's correlation coefficient!
