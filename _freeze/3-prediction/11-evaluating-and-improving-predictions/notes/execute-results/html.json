{
  "hash": "c7572320ae57ce26d809d6ea2594e902",
  "result": {
    "markdown": "---\ntitle: \"Evaluating and Improving Predictions\"\nsubtitle: \"$R^2$, residual analysis, and dummy variables.\"\ndate: \"09/22/2022\"\nimage: images/poverty.png\nformat:\n  html:\n    code-fold: true\n    code-link: true\n    code-summary: \".\"\nexecute: \n  warning: false\n  message: false\n---\n\n\n[[Discuss]{.btn .btn-primary}](https://edstem.org) [[Reading Questions]{.btn .btn-primary}](https://www.gradescope.com/courses/416233)\n\n\\\n\n[I]{.dropcap}n the last lecture we built our first prediction machine: an equation of a line drawn through the scatter plot.\n\n$$ \\hat{y} = 96.2 + -0.89 x $$\n\nWhile the idea is simple enough, there is a sea of terminology that floats around this method. A *linear model* is any model that predicts the $y$, often called the *response variable* or *dependent variable*, as a linear function of the $x$, often called the *predictor* or *independent variable* or *explanatory variable*. There are many different methods that can be used to decide which line to draw through a scatter plot. The most commonly-used approach is called the *method of least squares* which selects the line that minimizes the residual sum of squares (RSS). If we zoom out, a linear model fit by least squares is an example of a *regression model*, which refers to any model used to predict a numerical response variable.\n\nThe reason for all of this jargon isn't purely to infuriate students of statistics. Regression models for prediction are used in biology, finance, Each field that uses them tends to adapt particular falvors\n\nIn these lecture notes we focus on two questions: How can we evaluate the quality of our predictions? and How can we improve them?\n\n## Evaluating the fit to your data\n\nOnce you have fit a linear model to a scatter plot, you are able to answer questions such as:\n\n> What graduation rate would you expect for a state with a poverty rate of 15%?\n\nGraphically, this can be done by drawing a vertical line from where the poverty rate is 15% and finding where that line intersects your linear model. If you trace from that intersection point horizontally to the y-axis, you'll find the predicted graduation rate.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\npoverty <- read_csv(\"https://tinyurl.com/stat20poverty\")\n\np1 <- ggplot(poverty, aes(Poverty, Graduates)) + \n    xlim(0, 20) +\n    ylim(75, 96) +\n    geom_point() +\n    theme_bw()\n\nm1 <- lm(Graduates ~ Poverty, data = poverty)\n\npovnew <- data.frame(Poverty = 15)\nyhat <- predict(m1, povnew)\n\np1 + \n  geom_abline(intercept = m1$coef[1], slope = m1$coef[2], \n              col = \"goldenrod\") +\n  geom_vline(xintercept = 15, color = \"steelblue\", lty = 2) +\n  geom_hline(yintercept = yhat, color = \"steelblue\", lty = 2)\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nFrom the plot above, we can tell that the model yields a prediction around roughly 82.5%. To be more precise, we could plug the x-value into our equation for the line and solve.\n\n$$ \\hat{y} = 96.2 + -0.89 \\cdot 15 = 82.85 $$\n\nSo how good of a prediction is 82.85%? Until we observe a state with a poverty rate of 15%, we'll never know! What we *can* know, however, is how well our model explains the structure found in the data that we *have* observed. For those observations, we have both the predicted (or fitted) values $\\hat{y}_i$ as well as their actual y-values $y_i$. These can be used to calculate a statistic that measures the explanatory power of our model.\n\n### Measuring explanatory power: $r^2$\n\n$r^2$ is a statistic that captures how good the predictions from your linear model are ($\\hat{y}$) by comparing them another even simpler model: $\\bar{y}$. To understand how this statistic is constructed please watch this short video found in the Media Gallery on bCourses.\n\n- [R^2](https://bcourses.berkeley.edu/courses/1517492/external_tools/78985) (14 minutes)\n\n![](images/r-squared.png){fig-align=center width=400}\n\n*R-squared ($r^2$)*\n\n:    A statistic that measures the proportion of the total variability in the y-variable (total sum of squares, TSS) that is explained away using our model involving x (sum of squares due to regression, SSR).\n\n     $$ R^2 = \\frac{SSR}{TSS} = \\frac{\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^n (y_i - \\bar{y_i})^2}$$\n\n     $r^2$ has the following properties:\n\n     1. Always takes values between 0 and 1.\n     2. $r^2$ near 1 means predictions were more accurate.\n     3. $r^2$ near 0 means predictions were less accurate.\n\n\n#### Example: Poverty and Graduation\n\nLast lecture we showed how to use the `lm()` function to fit a linear model to predict the graduation rate in a US state using the poverty rate.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(tidyverse)\npoverty <- read_csv(\"https://tinyurl.com/stat20poverty\")\n\nm1 <- lm(Graduates ~ Poverty, data = poverty)\n```\n:::\n\n\nWhen you run this code, you'll see a new object appear in your environment: `m1`. This new object, though, is not a vector or a data frame. It's a much richer object called a *list* that stores all sorts of information about your linear model. You can click through the different part of `m1` in your environment pane, or your can use functions from the `broom` package to extract the important components using code.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(broom)\n\nglance(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.558        0.549  2.50    61.8 3.11e-10     1  -118.  242.  248.    307.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n```\n:::\n:::\n\n\nThe `glance()` function returns a series of different metrics used to evaluate the quality of your model. First among those is r-squared. Because the output of `glance()` is just another data frame, we can extract just the r-squared column using `select()`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nglance(m1) %>%\n    select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.558\n```\n:::\n:::\n\n\nWe learn that the linear model using the poverty rate is able to explain about 56% of the variability found in graduation rates. That's a good start!\n\n### Residual analysis\n\nA statistic like $r^2$ summarizes the explanatory power of our model in a single number. It's also important to understand *where* our model is performing well and where it is performing poorly. This is the the goal of residual analysis.\n\nRecall that a residual is defined as the difference between an observed y-value, $y_i$, and the expected y-value under the model, $\\hat{y_i}$. A data set that has $n$ observations will have $n$ values of $y_i$, $n$ values of $\\hat{y}_i$, and therefore $n$ residuals.\n\nWe can add the vector of residuals to our original data frame by using the `augment()` function inside `broom`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\naugment(m1, poverty)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 51 × 9\n   State            Poverty Gradu…¹ .fitted .resid   .hat .sigma .cooksd .std.…²\n   <chr>              <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>\n 1 Alabama             14.6    79.9    83.1 -3.19  0.0416   2.48 3.69e-2 -1.30  \n 2 Alaska               8.3    90.6    88.7  1.85  0.0390   2.51 1.15e-2  0.754 \n 3 Arizona             13.3    83.8    84.3 -0.460 0.0275   2.53 4.92e-4 -0.186 \n 4 Arkansas            18      80.9    80.0  0.860 0.112    2.53 8.36e-3  0.365 \n 5 California          12.8    81.1    84.7 -3.61  0.0240   2.47 2.62e-2 -1.46  \n 6 Colorado             9.4    88.7    87.8  0.938 0.0275   2.53 2.04e-3  0.380 \n 7 Connecticut          7.8    87.5    89.2 -1.70  0.0458   2.52 1.16e-2 -0.695 \n 8 Delaware             8.1    88.7    88.9 -0.229 0.0416   2.53 1.90e-4 -0.0935\n 9 District of Col…    16.8    86      81.1  4.88  0.0815   2.42 1.84e-1  2.04  \n10 Florida             12.1    84.7    85.3 -0.637 0.0208   2.53 7.03e-4 -0.257 \n# … with 41 more rows, and abbreviated variable names ¹​Graduates, ²​.std.resid\n```\n:::\n:::\n\n\nWe see that augment has added several columns to our data frame, each one prefixed with a `.`. We can again used `select()` to retain just the columns that we're interested in, $\\hat{y}_i$ and $y_i$, and save it back into our original data frame so that we can those columns for later.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\npoverty <- augment(m1, poverty) %>%\n    select(State, Poverty, Graduates, .fitted, .resid)\n```\n:::\n\n\nThe residuals capture where our model came up short: how far off the predictions were from reality. We can get an overall sense of their distribution by using a technique familiar from the last unit: a histogram.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nggplot(poverty, aes(x = .resid)) +\n    geom_histogram()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nWe see a distribution that is reasonably symmetric. There are a few states, however, where our prediction was off by nearly 5 percentage points. Let's identify those states by creating a filter to extract all states with predictions that were off by at least 5.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\npoverty %>%\n    filter(.resid > 5 | .resid < -5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  State        Poverty Graduates .fitted .resid\n  <chr>          <dbl>     <dbl>   <dbl>  <dbl>\n1 Montana         13.7      90.1    83.9   6.20\n2 Rhode Island    10.3      81      87.0  -5.95\n3 Texas           15.3      77.2    82.5  -5.26\n```\n:::\n:::\n\n\nWhy do you think our model did poorly with these states in particular? Are they distinctive in some way in terms of their system of education or economic structure? I don't have answers to those questions, but they highlight an important role of residual analysis. The process of building a predictive model doesn't stop once you have the equation for your line. Residual analysis helps highlight where your model is missing the mark, generating questions that can help you improve and better understand your model.\n\nBeyond looking at the residuals in isolation, we can investigate whether there is a systematic trend in where those residuals are occurring. We can explore the relationship between the residuals of each observation and their poverty rate by plotting the two columns against one another in a scatter plot.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(poverty, aes(x = Poverty,\n                    y = .resid)) +\n    geom_point()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nFrom this we learn that, in general, our residuals were smaller (and thus our predictions more accurate) for states that had lower poverty levels. At higher poverty levels (around 15%) the residuals were becoming larger in magnitude, both positive and negative.\n\nTo recap, residual analysis is like conducting a post-mortem on your model. It helps you understand where the model failed and often helps generates ideas to make your next model better. Here we discussed three techniques for residual analysis:\n\n1. Plot the distribution of the residuals to understand their shape.\n2. Investigate observations with very high and very low residuals.\n3. Plot the residuals against other variables in the data set to understand systematic successes and failures of your model.\n\n## Improving predictions\n\nSo far our predictive model has been very simple: it has used just one explanatory variable to make predictions, and we've made those predictions using one of the simplest geometric forms: a line. In the real world, though, we often have more than just one explanatory variable at our disposal.\n\nIn the video below we build a simple model to predict the weight of a book based on just one other variable.\n\n- [Improving Predictions I](https://bcourses.berkeley.edu/courses/1517492/external_tools/78985) (6 minutes)\n\nAs the video shows, we can fit the model using `lm()` and then use `summary()` to get an overview of our several aspects of our model.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nbooks <- read_csv(\"https://tinyurl.com/stat20books\")\n\nm1 <- lm(weight ~ volume, data = books)\n```\n:::\n\n\nTo extract just the r-squared value, we can use glance.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nglance(m1) %>%\n    select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.803\n```\n:::\n:::\n\n\nThat's a good start, but we can do better. Pick up the lecture in part II to build our first example of a predictive model that uses more than one variable.\n\n- [Improving Predictions II](https://bcourses.berkeley.edu/courses/1517492/external_tools/78985) (13 minutes)\n\nWe learn we can improve predictions by adding in additional variables, including categorical variables like `cover`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nm2 <- lm(weight ~ volume + cover, data = books)\nsummary(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = weight ~ volume + cover, data = books)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-110.10  -32.32  -16.10   28.93  210.95 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  197.96284   59.19274   3.344 0.005841 ** \nvolume         0.71795    0.06153  11.669  6.6e-08 ***\ncoverpb     -184.04727   40.49420  -4.545 0.000672 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 78.2 on 12 degrees of freedom\nMultiple R-squared:  0.9275,\tAdjusted R-squared:  0.9154 \nF-statistic: 76.73 on 2 and 12 DF,  p-value: 1.455e-07\n```\n:::\n:::\n\n\nThis changes the geometry of the model - we're now predicting from two parallel lines, not one single line - and it also changes the value of $r^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nglance(m2) %>%\n    select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.927\n```\n:::\n:::\n\n\nBy adding in the information on whether the book was hardback or paperback we were, able to improve the explanatory power of our model from 80% to 93%. That's a good progress!\n\nThis is just the tip of the iceberg for how powerful these models can grow at making predictions. In the next lecture we'll beyond one and two predictor variables to many predictor variables.",
    "supporting": [
      "notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}