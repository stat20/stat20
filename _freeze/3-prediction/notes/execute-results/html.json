{
  "hash": "0ca12a6001b6d4419343a3c5ce40023c",
  "result": {
    "markdown": "---\ntitle: \"Prediction\"\nsubtitle: \"TODO.\"\ndate: \"09/23/2022\"\nimage: images/penguin-plot.png\nformat:\n  html:\n    code-fold: true\n    code-link: true\n    code-summary: \".\"\nexecute: \n  warning: false\n  message: false\n---\n\n\nA predictive algorithm is a function that maps some input to some output. Take a look at the figure below. What might our predictive function look like that would guess body mass from flipper_length_mm?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\n# set the random seed so the plot always look the same\n\npenguins %>% \n    ggplot(aes(x=flipper_length_mm, y=body_mass_g)) +\n    geom_point() \n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n# Predictive algorithm = some function\n\nA predictive algorithm is just a mathematical function so let's make some guesses.\n\n-   $f(x) = .3$ (constant function)\n-   $f(x) = 1.2 \\cdot x + 2.1$ (linear function)\n-   $f(x) = \\sin(x)$ (sinusoidal function)\n-   $f(x) =$ the number of likes the $x$th most popular ticktok today had (not shown)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cowplot)\n\nexample_pred_const <- function(x){\n    value = 3000\n    rep(value, length(x))\n}\n\nexample_pred_linear <- function(x){\n    50 * x - 5000\n}\n\nexample_pred_sin <- function(x){\n    3000 * sin((2 * pi / 50) * x) + 4000\n}\n\n\n# setup data frame with silly predictions\npenguins_with_silly_pred <- penguins %>% \n    mutate(y_lin=example_pred_linear(flipper_length_mm),\n           y_const=example_pred_const(flipper_length_mm),\n           y_sin = example_pred_sin(flipper_length_mm))\n\n\n\nplot_with_const_pred <- penguins_with_silly_pred %>%\n        ggplot(aes(x=flipper_length_mm, y=body_mass_g)) +\n        geom_point() +\n        geom_line(aes(x=flipper_length_mm, y=y_const), color='red') +\n        ggtitle(\"Constant prediction function\")\n\nplot_with_lin_pred <- penguins_with_silly_pred %>%\n        ggplot(aes(x=flipper_length_mm, y=body_mass_g)) +\n        geom_point() +\n        # lims(x=c(-4, 4), y=c(-4, 4)) +\n        geom_line(aes(x=flipper_length_mm, y=y_lin), color='red') + \n        ggtitle(\"Linear prediction function\")\n\nplot_with_sin_pred <- penguins_with_silly_pred %>%\n        ggplot(aes(x=flipper_length_mm, y=body_mass_g)) +\n        geom_point() +\n        geom_line(aes(x=flipper_length_mm, y=y_sin), color='red') +\n        ggtitle(\"Sinusoidal prediction function\")\n\n\n\nplot_grid(plot_with_const_pred, plot_with_lin_pred, plot_with_sin_pred,\n          ncol=3, nrow=1)\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n### Digression: functions in R\n\nYou can write your own function in R as follows\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nname_of_my_function <- function(input1, intput2){\n    # Do stuff\n    \n    # last line gets returned\n}\n```\n:::\n\n\nFor example\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nadd_two_numbers <- function(input1, input2){\n    input1 + input2\n}\n\n# lets try the function! Here we use the argument names explicily, but we dont have to \nadd_two_numbers(input1=1, input2=1) # add_two_numbers(1, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlinear_predictor <- function(x){\n    1.2 * x + 2.1\n}\n\n# notice this function takes a vector and spits out a vector\nx_values <- c(1, 2, 3, 5)\nlinear_predictor(x_values)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.3 4.5 5.7 8.1\n```\n:::\n:::\n\n\n# Learning from data to build better predictive algorithms\n\nOk clearly we can do better than the functions we showed above. From visual inspection of the scatter plot we can probably guess a linear function would be best. Recall a linear function is *parameratized* by its *slope* and *intercept,*\n\n\n$$\nf(x) = \\text{slope} * x + \\text{intercept}\n$$\n\n\nWhat should the slope and intercept be for the figure above? We need to look at the data to figure out the best values for these *model parameters*.\n\nTo determine what the best model parameters are we need a way to evaluate how well a given model fits our data. The plot below shows two potential lines to fit the data. We can all agree the red line is a better fit, but how would we quantify that?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# TODO: show plots\nget_linear_model_pred <- function(x_vals, slope, intercept){\n    slope * x_vals + intercept\n}\n\n\n\npenguins_with_2_lines <- penguins %>% \n    mutate(y_pred_bad=get_linear_model_pred(x_vals=flipper_length_mm,\n                                            slope=40,\n                                            intercept = -5000),\n           y_pred_better=get_linear_model_pred(x_vals=flipper_length_mm,\n                                               slope=50,\n                                               intercept = -5700))\n    \n\n\npenguins_with_2_lines %>% \n    ggplot(aes(x=flipper_length_mm, y=body_mass_g)) +\n    geom_point() +\n    geom_line(aes(x=flipper_length_mm, y=y_pred_bad), color='blue') + \n    geom_line(aes(x=flipper_length_mm, y=y_pred_better), color='red')\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n# Quantifying goodness of fit with predicted residuals\n\nFor a given $(x, y)$ data point and a prediction function $f(x)$ a natural measure of fit comes from the residuals (recall the residuals in a previous lecture \\[TODO: link\\]). In particular, either the *absolute residual* $|y - f(x)|$ or *squared residual* $(y - f(x))^2$. We can the measure the overall fit of a function through the typical residual value\n\n\n$$\n\\text{Godness-of-fit}(f, \\{x_i\\}_{i=1}^n, \\{y_i\\}_{i=1}^n) = \\text{typical}\\left[\\text{residual}(y_1, f(x_1), \\dots, \\text{residual}(y_n, f(x_n) \\right] \n$$\n\n\n:::{.column-margin}\nThe notation $\\{x_i\\}_{i=1}^n$ is just short hand for the collection of observations $x_1, x_2, \\dots, x_n$.\n:::\n\n\nFor example the *median absolute residual*\n\n\n$$\n\\text{median}\\left( |y_1 - f(x_1)|, \\dots, |y_n - f(x_n)| \\right) \n$$\n\n\nor the *mean residuals sum of squares*\n\n\n$$\n\\frac{(y_1 - f(x_1))^2 + \\dots + (y_n - f(x_n))^2}{n}\n$$\n\n\nWhile the former seems appealing (e.g. for robustness), we almost always use the latter for pragmatic computational reasons.\n\nLets see what this looks like visually\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_with_2_lines %>% \n    ggplot(aes(x=flipper_length_mm, y=body_mass_g)) +\n    geom_point() +\n    geom_line(aes(x=flipper_length_mm, y=y_pred_bad), color='blue') + \n    geom_segment(aes(xend = flipper_length_mm, yend = y_pred_bad), color='blue', alpha=.1)\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\npenguins_with_2_lines %>% \n    ggplot(aes(x=flipper_length_mm, y=body_mass_g)) +\n    geom_point() +\n    geom_line(aes(x=flipper_length_mm, y=y_pred_better), color='red') + \n    geom_segment(aes(xend = flipper_length_mm, yend = y_pred_better), color='red', alpha=.1)\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\nReturning to the two functions above we can calculate our goodness of fit measures for the two lines.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# this data frame was created in the code that's hidden by default above\npenguins_with_2_lines %>% \n    mutate(residual_bad = y_pred_bad - body_mass_g,\n           residual_better = y_pred_better - body_mass_g) %>% \n    summarise(avg_rss_bad=mean(residual_bad^2, na.rm=T),\n              avg_rss_better=mean(residual_better^2, na.rm=T))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 2\n  avg_rss_bad avg_rss_better\n        <dbl>          <dbl>\n1    1530607.        175303.\n```\n:::\n:::\n\n\nLo and behold the red line has a smaller mean residual sum of squares!\n\n# Learning the best fit line with `lm()`\n\nWe're now armed with everything we need to find the line of best fit. Here the line of best fit is the line (slope+intercept) that has the smallest residual sum of squares for our dataset. It's just a calculus exercise to derive what the best slope/intercept would be. Fortunately R has a built in function to do this for us (and much more).\n\nWe can fit a *linear model* in R with the `lm()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlinear_model <- lm(body_mass_g ~ flipper_length_mm, data=penguins)\nlinear_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n\nCoefficients:\n      (Intercept)  flipper_length_mm  \n         -5780.83              49.69  \n```\n:::\n:::\n\n\nThe coefficient and intercept can be found as\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlinear_model$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      (Intercept) flipper_length_mm \n      -5780.83136          49.68557 \n```\n:::\n:::\n\n\nWe can use the `linear_model` object to obtain predictions (it would not have been too painful to do this ourselves)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ny_pred <- predict(linear_model, penguins)\ny_pred[1:10] # just print first 10 values!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3        4        5        6        7        8 \n3212.256 3460.684 3907.854       NA 3808.483 3659.426 3212.256 3907.854 \n       9       10 \n3808.483 3659.426 \n```\n:::\n:::\n\n\n### Interpreting the linear regression coefficient\n\nThe `slope` of means for every one unit change in x we expect to see y change by `slope` units. The `intercept` means that if x is 0 then we expect y to be around the `intercept`.\n\n# Multiple predictors\n\nMost of the time we have more than one x variable (*predictor* or *covariate* or *feature)*. Suppose we arrange our variables in a vector $x = (x^{(1)}, \\dots, x^{(d)})$, then our linear prediction would look like\n\n\n$$\nf(x) = a_{\\text{intercept}} + c^{(1)} x^{(1)}+ \\dots + c^{(d)} x^{(d)}\n$$\n\n\nwhere $c=(c^{(1)}, \\dots, c^{(d)})$ is the *coefficients* (one for each variable) and $a_{\\text{intercept}}$ is of course the intercept. The coefficients play the same role as the slope when there was one variable.\n\nWe fit a linear model as\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlinear_model <- lm(body_mass_g ~ flipper_length_mm + bill_length_mm, \n                   data=penguins)\n\nlinear_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm, \n    data = penguins)\n\nCoefficients:\n      (Intercept)  flipper_length_mm     bill_length_mm  \n        -5736.897             48.145              6.047  \n```\n:::\n:::\n\n\nIf we have two variables we can still visualize the linear function; it is a plane living in 3 dimensions (the first two dimensions are the 2 variables, the 3rd is the y variable).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_no_nan <- penguins %>% \n        select(flipper_length_mm, bill_length_mm, body_mass_g) %>% \n        drop_na()\n\n# borrowing code from:\n# http://www.sthda.com/english/wiki/impressive-package-for-3d-and-4d-graph-r-software-and-data-visualization\n# https://github.com/idc9/stor390/blob/master/notes/linear_regression/linear_regression.Rmd\nlibrary(plot3D)\nx <- penguins_no_nan$flipper_length_mm\ny <- penguins_no_nan$bill_length_mm\nz <- penguins_no_nan$body_mass_g\n\nfit <- lm(z ~ x + y)\n# predict values on regular xy grid\ngrid.lines = 26\nx.pred <- seq(min(x), max(x), length.out = grid.lines)\ny.pred <- seq(min(y), max(y), length.out = grid.lines)\nxy <- expand.grid( x = x.pred, y = y.pred)\nz.pred <- matrix(predict(fit, newdata = xy), \n                 nrow = grid.lines, ncol = grid.lines)\nfitpoints <- predict(fit)\n\n# scatter plot with regression plane\nscatter3D(x, y, z, pch = 19, cex = .5, alpha=.4, col='red', \n          theta = 200, phi =25, ticktype = \"detailed\",\n          xlab = \"flipper_length_mm\", ylab = \"bill_length_mm\", zlab = \"body_mass_g\",  \n          surf = list(x = x.pred, y = y.pred, z = z.pred, facets = NA, alpha=1, col='black', fit=fitpoints), \n          main = \"linear regression with 2 predictors\")\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nIf we have $d >= 3$ two variables our prediction function would be a [hyperplane](https://en.wikipedia.org/wiki/Hyperplane) living in $d + 1$ dimensional space. It's a bit harder to visualize things in 4 or more dimensions, but it's analogous to a 2d plane living in 3 dimensions.\n",
    "supporting": [
      "notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}