{
  "hash": "ded339cbe85d0428265975bf9d53bc6c",
  "result": {
    "markdown": "---\ntitle: \"Summarizing numerical associations\"\nsubtitle: \"Covariance, Pearson correlation, and a taste of non-linear correlation measures\"\ndate: \"09/20/2022\"\nformat:\n  html:\n    code-fold: true\n    code-link: true\n    code-summary: \".\"\nexecute: \n  warning: false\n  message: false\n---\n\n\n[[Discuss]{.btn .btn-primary}](https://edstem.org) [[Reading Questions]{.btn .btn-primary}](https://www.gradescope.com/courses/416233)\n[[PDF]{.btn .btn-primary}](notes.pdf)\n\n\\\n\nHow would you describe the structure that you see in these two scatter plots?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# set the random seed so the plot always look the same\nset.seed(1) \n\nn_samples <- 300\nsigma_ident <- matrix(c(1,0,0,1), 2, 2)\nsigma_is_corr <- matrix(c(1,.8,.8, 1), 2, 2)\n\ndf_corr <- mvrnorm(n = n_samples, \n                   mu = c(0, 0), \n                   Sigma = sigma_is_corr) %>% \n    as_tibble() %>% \n    setNames(c('x', 'y')) %>%\n    mutate(index = \"B\")\n\ndf_indep <- data.frame(x = df_corr$x,\n                       y = sample(df_corr$y),\n                       index = \"A\")\n\nplot_indep <- df_indep %>%\n    ggplot(aes(x = x,\n               y = y)) + \n    geom_point() + \n    lims(x = c(-4, 4),\n         y = c(-4, 4)) +\n    labs(subtitle = \"A\") +\n    theme_bw()\n\nplot_corr <- df_corr %>%\n    ggplot(aes(x = x, \n               y = y)) +\n    geom_point() +\n    lims(x = c(-4, 4), \n         y = c(-4, 4)) +\n    labs(subtitle = \"B\") +\n    theme_bw()\n\nplot_indep + plot_corr\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=744}\n:::\n:::\n\n\nYou might say that plot A can shows no particular association between the x and the y, unlike plot B. Plot B displays an association that can be described in terms of its *shape*, *direction*, and *strength*. The shape is straightforward: it looks like a linear relationship. The direction is too: this is a positive relationship. What about its strength? Is it weak? Moderate? Strong? Very strong?\n\nWe've seen the notion of \"association\" come up before in the context of Summarizing Categorical Data.  Let's generalize that definition to also encompass the numerical data that we see in these scatter plots.\n\n**Association**\n:    There is an association between two variables if the conditional distribution of one varies as you move across values of the other.\n\nVisually, you can see this by conditioning on a particular interval of the x axis, taking a vertical strip of points within that interval, and considering the distribution of their y coordinate. If you repeat this process for plot B three times, taking on vertical strip of points from the left side of the plot, one from the center, one from the right, the conditional distribution of the y coordinates is steadily shifting upwards. That's what characterizes the association we see in plot B. By contrast, if we repeated this procedure for plot A, the distribution of the y would stay rooted around 0.\n\nAssigning adjectives like \"weak\", \"moderate\", and \"strong\" to describe the strength of an association is helpful, but it becomes particularly powerful when it is quantified with a statistic. At this point we are familiar with several statistics to measure the center and spread of a numerical variable. These are *univariate* statistics because they are calculated on just one variable. We can calculate the means and standard deviations of the x and the y variables in each of the scatter plots above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- bind_rows(df_indep, df_corr)\n\ndf %>%\n    group_by(index) %>%\n    summarize(mean_x = mean(x),\n              mean_y = mean(y),\n              sd_x = sd(x),\n              sd_y = sd(y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  index mean_x mean_y  sd_x  sd_y\n  <chr>  <dbl>  <dbl> <dbl> <dbl>\n1 A     0.0352 0.0285 0.965 0.979\n2 B     0.0352 0.0285 0.965 0.979\n```\n:::\n:::\n\n\nThey are . . . identical?\n\nIt's puzzling but true: if you look at the tick marks on the x and y axes, both scatter plots share the same center and spread for both variables. What we need is a statistic to capture what is obvious to our eye: that plot B has an association between the x and y while plot A does not. This will need to be a statistic calculated from two variables, our first *bivariate* statistic. There are many statistics that could be constructed to serve this purpose. Here we look at a few of the most common.\n\n\n## Covariance\n\nIf you were asked for a mathematical formula for the \"typical value\" of a variable you could probably come up with the mean or median. Similarly, if you were asked for a formula for the \"typical distance to the typical value\" you could probably derive the variance or [median absolute deviation](https://en.wikipedia.org/wiki/Median_absolute_deviation). The covariance formula is not quite so immediately discoverable.\n\nInstead of trying to quantify any possible association between two variables let's restrict our attention to *linear associations* for now. In other words, if our first variable increases, the second variable increases proportionally. What would we want out of a bivariate association statistic?\n\n::: {.callout-warning appearance=\"simple\" icon=\"false\"}\n## Desiderata\n\n-   Large positive when there is a strong increasing association\n-   Large negative when there is a strong decreasing association\n-   Close to 0 when there is no association\n:::\n\n#### Digression: residuals\n\nSuppose we observe $n$ observations of a variable $x_1, \\dots, x_n$. The residuals quantify how far each observation is from the typical value. In detail, the residuals from the mean are $x_1 - \\overline{x}$, $x_2 - \\overline{x}$, ..., $x_n - \\overline{x}$ where we [recall](https://www.stat20.org/2-describing-data/05-summarizing-numerical-data/notes.html#constructing-numerical-summaries) $\\overline{x}$ is the mean.\n\n### Sample Covariance\n\nSuppose we make $n$ observations of two variables $x_1, \\dots, x_n$ and $y_1, \\dots, y_n$ and we want to quantify the association between x and y. One of the standard statistics that is used is called the *sample covariance*.\n\n$$\n\\text{cov(x, y)} = \\frac{(x_1 - \\overline{x})(y_1 - \\overline{y}) + (x_2 - \\overline{x})(y_2 - \\overline{y}) + \\dots + (x_n - \\overline{x})(y_n - \\overline{y})}{n- 1}\n$$\n\n:::{.column-margin}\nLike sample variance, sample covariance formula uses `n-1` instead of an `n` in the denominator for technical reasons not worth getting into at this point. When `n` is large it doesn't really make a difference which one you use. \n\nIn R, the relevant function is `cov(x, y)`.\n:::\n\nIn other words the sample covariance is the \"the typical product of x and y residuals\".\n\n\nIt may not be immediately clear how this covariance formula satisfies our desiderata. When we want to understand a complex mathematical object it is often helpful to consider special cases.\n\n:::{style=\"text-align: center\"}\n**Special case 1:** $x_1 = 0, y_1 = 0, x_2 = 1, y_2 = 1$\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata.frame(c(0, 1), c(0, 1)) %>% \n    setNames(c('x', 'y')) %>%  \n    ggplot(aes(x = x, \n               y = y)) +\n    geom_point() + \n    lims(x = c(0, 1), \n         y = c(-1, 1)) +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=316.8}\n:::\n:::\n\n\nIn this example, $\\overline{x} = \\frac{1}{2}$ and $\\overline{y} = \\frac{1}{2}$. We can plug these into the covariance formula:\n\n$$\n\\text{cov} = \\frac{(0 - \\frac{1}{2})(0 - \\frac{1}{2}) + (1 - \\frac{1}{2})(1 - \\frac{1}{2})}{2} = \\frac{\\frac{1}{4} + \\frac{1}{4}}{2 - 1} = 0.5\n$$\n\ni.e. the covariance is positive.\n\n:::{style=\"text-align: center\"}\n**Special case 2:** $x_1 = 0, y_1 = 0, x_2 = 1, y_2 = -1$\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata.frame(c(0, 1), c(0, -1)) %>% \n    setNames(c('x', 'y')) %>%  \n    ggplot(aes(x = x, \n               y = y)) +\n    geom_point() + \n    lims(x = c(0, 1), \n         y = c(-1, 1)) +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=316.8}\n:::\n:::\n\n\nHere $\\overline{x} = \\frac{1}{2}$ and $\\overline{y} = -\\frac{1}{2}$, which we plug into the covariance formula:\n\n$$\n\\text{cov} = \\frac{(0 - \\frac{1}{2})(0 - -\\frac{1}{2}) + (1 - \\frac{1}{2})(-1 - -\\frac{1}{2})}{2} = \\frac{-\\frac{1}{4} + -\\frac{1}{4}}{2 - 1} = -0.5\n$$\n\ni.e. the covariance is negative.\n\n\nSome takeaways from these calculations.\n\n-   If our data points are typically  linearly increasing then the terms in the covariance formula will be mostly positive and the covariance will be a large positive value.\n\n-   If the points are typically linearly decreasing then the covariance will be a large negative value.\n\n-   If each pair of x/y terms are randomly increasing or decreasing then about half the terms in the formula will be positive and half will be negative; here the covariance will be close to 0.\n\n\n### Example\n\n> Is a child's height more associated with their mother or father's height?\n\nTo answer this, lets turn to data from a survey of childrens' heights and their parents' heights, all measured in inches.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(HistData)\ndata(GaltonFamilies)\nfamily_heights <- tibble(GaltonFamilies)\nfamily_heights %>%\n    select(father, mother, childHeight)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 934 × 3\n   father mother childHeight\n    <dbl>  <dbl>       <dbl>\n 1   78.5   67          73.2\n 2   78.5   67          69.2\n 3   78.5   67          69  \n 4   78.5   67          69  \n 5   75.5   66.5        73.5\n 6   75.5   66.5        72.5\n 7   75.5   66.5        65.5\n 8   75.5   66.5        65.5\n 9   75     64          71  \n10   75     64          68  \n# … with 924 more rows\n```\n:::\n:::\n\n\nLet's explore the relationship between heights by constructing two scatter plots, each with a different x variable.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np_dad <- family_heights %>% \n    ggplot(aes(x = father, \n               y = childHeight)) + \n    geom_jitter() + \n    xlab(\"Father height (inches)\") + \n    ylab(\"Child height (inches)\") +\n    theme_bw()\n\np_mom <- family_heights %>% \n    ggplot(aes(x = mother, \n               y = childHeight)) + \n    geom_jitter() + \n    xlab(\"Mother height (inches)\") + \n    ylab(\"Child height (inches)\") +\n    theme_bw()\n\np_dad + p_mom\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=744}\n:::\n:::\n\n\nIt's not exactly clear which association is stronger. Let's compare covariances.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfamily_heights %>% \n    summarise(father_cov = cov(father, childHeight),\n              mother_cov = cov(mother, childHeight))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  father_cov mother_cov\n       <dbl>      <dbl>\n1       2.36       1.65\n```\n:::\n:::\n\n\nAs measured by the covariance, child height is more strongly associated with father height.\n\n\n## Pearson correlation: a rescaled version of covariance\n\nThere is something unsatisfactory about covariance; it depends on the scale of the variables. Suppose we had instead measured mothers' heights in femtometers (femtometer = $10^{-15}$ meters).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# 2.54 cm/in, 10^13 femtometers/cm\nfamily_heights <- family_heights %>% \n        mutate(mother_femtometers = mother * 2.54 * 10^13)\n\n\nfamily_heights %>% \n    summarise(father_cov = cov(father, childHeight),\n              mother_cov = cov(mother_femtometers, childHeight))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  father_cov mother_cov\n       <dbl>      <dbl>\n1       2.36    4.19e13\n```\n:::\n:::\n\n\nSuddenly the covariance between child height (inches) and mother height (femtometers) is ridiculously large! The issue here is that covariance depends on the scale of the variables. For example, switching from inches to femtometers multiplies the mother's height variable by $2.54 \\cdot 10^{13}$. But changing the unit of measurement should not change the strength of association!\n\n::: {.callout-warning appearance=\"simple\" icon=\"false\"}\n## Additional desiderata for linear bivariate association\n\n-   Scale free; multiplying a variable by a (positive) number does not change the association.\n:::\n\n#### A digression: scaled residuals\n\nOne data intrinsic measure of scale is the standard deviation (the square root of the variance). We can make our data residuals scale free by dividing by the standard deviation i.e.$\\frac{x_1 - \\overline{x}}{\\text{sd}(x)}, \\frac{x_2 - \\overline{x}}{\\text{sd}(x)}, \\dots$. These residuals now measure the distance from each observation to the typical observation *relative to the natural variability of the dataset.*\n\nOne major advantage of the scaled residuals is that multiplying the observations by any positive number does not change the relative residuals. The relative residuals are scale free.\n\n<!-- Perhaps ask why we want sqrt variance not variance i.e. the units-->\n\n### Pearson correlation formula\n\nThe correlation is simply the covariance using the scaled residuals.\n\n$$\n\\text{cor}(x, y) = \\frac{\\frac{(x_1 - \\overline{x})}{sd(x)} \\frac{(y_1 - \\overline{y})}{sd(y)}  + \n\\frac{(x_2 - \\overline{x})}{sd(x)} \\frac{(y_2 - \\overline{y})}{sd(y)} + \\dots}{n} = \\frac{\\text{cov(x, y)}}{\\text{sd}(x) \\text{sd}(y)} = r\n$$\n\nIt is a mathematical fact that the pearson correlation is always between -1 and 1. Moreover it does not depend on the units/scale of the x or y data!\n\nNow let's compare the association between the child height and parent height using correlation. \n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfamily_heights %>% \n    summarise(father_cor = cor(father, childHeight),\n              mother_cor = cor(mother, childHeight),\n              mother_cor_femto = cor(mother_femtometers, childHeight))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  father_cor mother_cor mother_cor_femto\n       <dbl>      <dbl>            <dbl>\n1      0.266      0.201            0.201\n```\n:::\n:::\n\n\nNotice the change of units does not affect the correlation!\n\n\n## Moving beyond linearity\n\nNot all associations are linear. Each of the plots below shows to variables that are associated. The first column shows a moderate association; the second column shows a stronger association; the third column shows a perfect association (we call this a *deterministic relationship*).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Setup data\nn_samples <- 200\n\nx_vals <- runif(n=n_samples, min=-1, max=1)\n\n\ny_lin_low <- x_vals + rnorm(n=n_samples, mean=0, sd=1)\ny_lin_med <- x_vals + rnorm(n=n_samples, mean=0, sd=.25)\ny_lin_det <- x_vals\n\ndeg <- 3\ny_mono_low <- x_vals^deg + rnorm(n=n_samples, mean=0, sd=1)\ny_mono_med <- x_vals^deg + rnorm(n=n_samples, mean=0, sd=.1)\ny_mono_det <- x_vals^deg\n\n\ny_nonlin_low <- cos(x_vals * (2 * pi )) + rnorm(n=n_samples, mean=0, sd=1)\ny_nonlin_med <- cos(x_vals * (2 * pi )) + rnorm(n=n_samples, mean=0, sd=.25)\ny_nonlin_det <- cos(x_vals * (2 * pi ))\n\ndf <- tibble(x_vals, \n             y_lin_low, y_lin_med, y_lin_det,\n             y_mono_low, y_mono_med, y_mono_det,\n             y_nonlin_low, y_nonlin_med, y_nonlin_det)\n\n# compute correlations\nlibrary(XICOR)\n\n\nx_vals_vec <- df[, 1][[1]]\n\ncorr_df <- tibble(method = character(),\n                  variable = character(),\n                  corr = numeric())\nfor (method in c('pearson', 'spearman', 'chatterjee')){\n    for (j in 2:dim(df)[2]){\n        col <- colnames(df)[j]\n        \n        y_vals_vec <- df[, j][[1]]\n        \n        if (method == 'chatterjee'){\n            corr_val <- calculateXI(x=x_vals_vec, y=y_vals_vec)\n        }else{\n            corr_val <- cor(x_vals_vec, y_vals_vec, method=method)\n        }\n        \n        corr_df <- corr_df %>% add_row(method=method, variable=col, corr=corr_val)\n    }\n}\n\n# make plots\ntitle_txt_size <- 5\nget_title <- function(corr_df, v){\n    \n    pc <- corr_df %>% filter(method=='pearson', variable==v) %>% .[[3]]\n    sc <- corr_df %>% filter(method=='spearman', variable==v) %>% .[[3]]\n    cc <- corr_df %>% filter(method=='chatterjee', variable==v) %>% .[[3]]\n    \n    \n    txt <- ''\n    txt <- paste0(txt, 'r = ', round(pc, 2))\n    # txt <- paste0(txt, ', spearman = ', round(sc, 2))\n    # txt <- paste0(txt, ', chatterjee = ', round(cc, 2))\n    txt\n}\n\nplot_lin_low <- df %>%\n    ggplot(aes(x=x_vals, y=y_lin_low)) +\n    geom_point() +\n    ggtitle(get_title(corr_df=corr_df, v='y_lin_low'))  + \n    theme(plot.title = element_text(size = title_txt_size)) + \n    labs(x='x', y='y') +\n    theme_bw()\n\nplot_lin_med <- df %>% \n    ggplot(aes(x=x_vals, y=y_lin_med)) +\n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_lin_med'))  + \n    theme(plot.title = element_text(size = title_txt_size)) + \n    labs(x='x', y='y') +\n    theme_bw()\n\nplot_lin_det <- df %>% \n    ggplot(aes(x=x_vals, y=y_lin_det)) + \n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_lin_det'))  + \n    theme(plot.title = element_text(size = title_txt_size)) + \n    labs(x='x', y='y') +\n    theme_bw()\n\nplot_mono_low <- df %>% \n    ggplot(aes(x=x_vals, y=y_mono_low)) + \n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_mono_low'))  + \n    theme(plot.title = element_text(size = title_txt_size)) + \n    labs(x='x', y='y') +\n    theme_bw()\n\nplot_mono_med <- df %>%\n    ggplot(aes(x=x_vals, y=y_mono_med)) + \n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_mono_med'))  + \n    theme(plot.title = element_text(size = title_txt_size)) + \n    labs(x='x', y='y') +\n    theme_bw()\n\nplot_mono_det <- df %>%\n    ggplot(aes(x=x_vals, y=y_mono_det)) + \n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_mono_det'))  + \n    theme(plot.title = element_text(size = title_txt_size)) + \n    labs(x='x', y='y') +\n    theme_bw()\n\nplot_nonlin_low <- df %>%\n    ggplot(aes(x=x_vals, y=y_nonlin_low)) +\n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_nonlin_low'))  + \n    theme(plot.title = element_text(size = title_txt_size)) + \n    labs(x='x', y='y') +\n    theme_bw()\n\nplot_nonlin_med <- df %>%\n    ggplot(aes(x=x_vals, y=y_nonlin_med)) + \n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_nonlin_med'))  + \n    theme(plot.title = element_text(size = title_txt_size)) + \n    labs(x='x', y='y') +\n    theme_bw()\n\nplot_nonlin_det <- df %>%\n    ggplot(aes(x=x_vals, y=y_nonlin_det)) +\n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_nonlin_det'))  + \n    theme(plot.title = element_text(size = title_txt_size)) + \n    labs(x='x', y='y') +\n    theme_bw()\n\n(plot_lin_low + plot_lin_med + plot_lin_det) /\n    (plot_mono_low + plot_mono_med + plot_mono_det) /\n    (plot_nonlin_low + plot_nonlin_med + plot_nonlin_det)\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=768}\n:::\n:::\n\n\nThe first row shows a linear association. The second row shows a non-linear but *monotonic* association. A monotonic positive association just means that if x goes up then y has to go up -- but it does not need to go up proportionally to the increase in x. A monotonic negative association means the opposite (x goes up, y goes down). Monotonic is a generalization of linear (linear is monotonic, but monotonic is not necessarily linear).\n\nThese scatter plots demonstrate that the (Pearson) correlation coefficient is designed to capture one particular form of association - linear association. It does a reasonably good job of detecting non-linear but monotonic associations. It utterly fails, however, at detecting the non-linear non-monotonic associations found in the lower right plot. What we need are slightly different statistics. We will discuss them in class.\n\n<!-- We can quantify a monotonic relationship with [Spearman's rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient). -->\n\n<!-- The third row shows a non-linear association that non-monotonic. Quantifying such general associations is not easy. In fact [statisticians are still inventing](https://arxiv.org/abs/1909.10140) ways of quantifying general relationships such as Chatterjee's correlation coefficient! -->\n",
    "supporting": [
      "notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}