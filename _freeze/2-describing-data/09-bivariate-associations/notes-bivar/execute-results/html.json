{
  "hash": "f9c3f25b82e2cd9b85e97659342a61f6",
  "result": {
    "markdown": "---\ntitle: \"Summarizing bivariate numerical associations\"\nsubtitle: \"Covariance, Pearson correlation, and a taste of non-linear correlation measures\"\ndate: \"09/21/2022\"\nformat:\n  html:\n    code-fold: true\n    code-link: true\n    code-summary: \".\"\nexecute: \n  warning: false\n  message: false\n---\n\n\nHere we are going to discuss ways of quantifying associations between two variables. In other words we are studying descriptive statistics for *bivariate associations*.\n\nLooking at the two plots below we can see\n\n-   There seems to be little to no association between x and y in the first plot\n\n-   There is a positive association between x any y in the second plot\n\nBut how would we quantify these statements?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(cowplot)\n\n# set the random seed so the plot always look the same\nset.seed(1) \n\nn_samples = 500\nSigma_ident = matrix(c(1,0,0,1), 2, 2)\nSigma_is_corr = matrix(c(1,.8,.8, 1), 2, 2)\n\n\nplot_indep <- mvrnorm(n=n_samples, mu=c(0, 0), Sigma=Sigma_ident) %>% \n    as_tibble() %>% \n    setNames(c('x', 'y')) %>% \n    ggplot(aes(x=x, y=y)) + \n    geom_point() + \n    lims(x=c(-4, 4), y=c(-4, 4)) \n\n\nplot_is_corr <- mvrnorm(n=n_samples, mu=c(0, 0), Sigma=Sigma_is_corr) %>% \n    as_tibble() %>% \n    setNames(c('x', 'y')) %>% \n    ggplot(aes(x=x, y=y)) +\n    geom_point() +\n    lims(x=c(-4, 4), y=c(-4, 4))\n\n\nplot_grid(plot_indep, plot_is_corr, labels=c(\"A\", \"B\"), ncol = 2, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](notes-bivar_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n# A first measure of bivariate linear association: covariance\n\nIf you were asked for a mathematical formula for the \"typical value\" of a variable you could probably come up with the mean or median. Similarly, if you were asked for a formula for the \"typical distance to the typical value\" you could probably derive the variance or [median absolute deviation](https://en.wikipedia.org/wiki/Median_absolute_deviation). The covariance formula is not quite so immediately discoverable.\n\nInstead of trying to quantify any possible association between two variables let's restrict our attention to *linear associations* for now. In other words, if our first variable increases, the second variable increases proportionally. What would we want out of a bivariate association statistic?\n\n::: {.callout-warning appearance=\"simple\" icon=\"false\"}\n## Desiderata\n\n-   Large positive when there is a strong increasing association\n-   Large negative when there is a strong decreasing association\n-   Close to 0 when there is no association\n:::\n\n### Digression: residuals\n\nSuppose we observe $n$ observations of a variable $x_1, \\dots, x_n$. The residuals quantify how far each observation is from the typical value. In detail, the residuals from the mean are $x_1 - \\overline{x}$, $x_2 - \\overline{x}$, ..., $x_n - \\overline{x}$ where we [recall](https://www.stat20.org/2-describing-data/05-summarizing-numerical-data/notes.html#constructing-numerical-summaries) $\\overline{x}$ is the mean.\n\n### Covariance formula\n\nTo make things concrete suppose we observe $n$ observations of two variables $x_1, \\dots, x_n$ and $y_1, \\dots, y_n$ and we want to quantify the association between x and y. The covariance formula is\n\n\n$$\n\\text{cov(x, y)} = \\frac{(x_1 - \\overline{x})(y_1 - \\overline{y}) + (x_2 - \\overline{x})(y_2 - \\overline{y}) + \\dots + (x_n - \\overline{x})(y_n - \\overline{y})}{n- 1}\n$$\n\n:::{.column-margin}\nThe covariance formula has an `n-1` instead of an `n` in the denominator for technical reasons not worth getting into at this point. When `n` is large it doesn't really make a difference which one you use. \n:::\n\nIn other words the covariance is the \"the typical product of x and y residuals\".\n\n\nDoing this in R\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nx <- c(1, 2, 3, 4, 5, 6, 7)\ny <- c(2, 4, 6, 8, 10, 12, 15)\ncov(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.833333\n```\n:::\n:::\n\n\n### Special cases to build intuition\n\nIt may not be immediately clear how this covariance formula satisfies our desiderata. When we want to understand a complex mathematical object it is often helpful to consider *special cases*.\n\nSpecial case 1: $x_1 = 0, y_1 = 0, x_2 = 1, y_2 = 1$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(c(0, 1), c(0, 1)) %>% \n    setNames(c('x', 'y')) %>%  \n    ggplot(aes(x=x, y=y)) +\n    geom_point() + \n    lims(x=c(0, 1), y=c(-1, 1))\n```\n\n::: {.cell-output-display}\n![](notes-bivar_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nHere $\\overline{x} = \\frac{1}{2}$ and $\\overline{y} = \\frac{1}{2}$ and plugging into the covariance formula\n\n\n$$\n\\text{cov} = \\frac{(0 - \\frac{1}{2})(0 - \\frac{1}{2}) + (1 - \\frac{1}{2})(1 - \\frac{1}{2})}{2} = \\frac{\\frac{1}{4} + \\frac{1}{4}}{2 - 1} = 0.5\n$$\n\ni.e. the covariance is positive.\n\nSpecial case 2: $x_1 = 0, y_1 = 0, x_2 = 1, y_2 = -1$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(c(0, 1), c(0, -1)) %>% \n    setNames(c('x', 'y')) %>%  \n    ggplot(aes(x=x, y=y)) +\n    geom_point() + \n    lims(x=c(0, 1), y=c(-1, 1))\n```\n\n::: {.cell-output-display}\n![](notes-bivar_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nHere $\\overline{x} = \\frac{1}{2}$ and $\\overline{y} = -\\frac{1}{2}$ and plugging into the covariance formula\n\n\n$$\n\\text{cov} = \\frac{(0 - \\frac{1}{2})(0 - -\\frac{1}{2}) + (1 - \\frac{1}{2})(-1 - -\\frac{1}{2})}{2} = \\frac{-\\frac{1}{4} + -\\frac{1}{4}}{2 - 1} = -0.5\n$$\n\n\ni.e. the covariance is negative.\n\n\nSome takeaways from these calculations.\n\n-   If our data points are typically  linearly increasing then the terms in the covariance formula will be mostly positive and the covariance will be a large positive value!\n\n-   If the points are typically linearly decreasing then the covariance will be a large negative value!\n\n-   If each pair of x/y terms are randomly increasing or decreasing then about half the terms in the formula will be positive and half will be negative; here the covariance will be close to 0!\n\n\n### Is a child's height more associated with their mother or father's height\n\nLet's load a survey of childrens' heights and their parents heights. Note height here is measured in inches.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in a csv file from a webpage\nurl <- 'https://raw.githubusercontent.com/data-8/textbook/main/assets/data/family_heights.csv'\nfamily_heights <- read_csv(url)\nfamily_heights\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 933 × 8\n   family father mother midparentHeight children childNum sex    childHeight\n   <chr>   <dbl>  <dbl>           <dbl>    <dbl>    <dbl> <chr>        <dbl>\n 1 1        78.5   67              75.4        4        1 male          73.2\n 2 1        78.5   67              75.4        4        2 female        69.2\n 3 1        78.5   67              75.4        4        3 female        69  \n 4 1        78.5   67              75.4        4        4 female        69  \n 5 2        75.5   66.5            73.7        4        1 male          73.5\n 6 2        75.5   66.5            73.7        4        2 male          72.5\n 7 2        75.5   66.5            73.7        4        3 female        65.5\n 8 2        75.5   66.5            73.7        4        4 female        65.5\n 9 3        75     64              72.1        2        1 male          71  \n10 3        75     64              72.1        2        2 female        68  \n# … with 923 more rows\n```\n:::\n:::\n\n\nNow we can visually inspect\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfamily_heights %>% \n    ggplot(aes(x=father, y=childHeight)) + \n    geom_point() + \n    xlab(\"Father height (inches)\") + \n    ylab(\"Child height (inches)\") \n```\n\n::: {.cell-output-display}\n![](notes-bivar_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfamily_heights %>% \n    ggplot(aes(x=mother, y=childHeight)) + \n    geom_point() + \n    xlab(\"Mother height (inches)\") + \n    ylab(\"Child height (inches)\") \n```\n\n::: {.cell-output-display}\n![](notes-bivar_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\nIt's not exactly clear which association is stronger. Let's compare covariances\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfamily_heights %>% \n    summarise(father_cov = cov(father, childHeight),\n              mother_cov = cov(mother, childHeight))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  father_cov mother_cov\n       <dbl>      <dbl>\n1       2.29       1.64\n```\n:::\n:::\n\n\n\n# Pearson correlation: a rescaled version of covariance\n\nThere is something unsatisfactory about covariance; it depends on the scale of the variables. Suppose we had measured mothers' heights in femtometers (femtometer = $10^{-15}$ meters).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# 2.54 cm/in, 10^13 femtometers/cm\nfamily_heights <- family_heights %>% \n        mutate(mother_femtometers=13 * mother * 2.54 * 10^13)\n\n\nfamily_heights %>% \n    summarise(father_cov = cov(father, childHeight),\n              mother_cov = cov(mother_femtometers, childHeight))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  father_cov mother_cov\n       <dbl>      <dbl>\n1       2.29    5.42e14\n```\n:::\n:::\n\n\nSuddenly the covariance between child height (inches) and mother height (femtometers) is ridiculously large! The issue here is that covariance depends on the scale of the variables. For example, switching from inches to femtometers multiplies the mother's height variable by $2.54 \\cdot 10^{13}$. But changing the unit of measurement should not change the strength of association!\n\n::: {.callout-warning appearance=\"simple\" icon=\"false\"}\n## Additional desiderata for linear bivariate association\n\n-   Scale free; multiplying a variable by a (positive) number does not change the association.\n:::\n\n### A digression: scaled residuals\n\nOne data intrinsic measure of scale is the standard deviation (the square root of the variance). We can make our data residuals scale free by dividing by the standard deviation i.e.$\\frac{x_1 - \\overline{x}}{\\text{sd}(x)}, \\frac{x_2 - \\overline{x}}{\\text{sd}(x)}, \\dots$. These residuals now measure the distance from each observation to the typical observation *relative to the natural variability of the dataset.*\n\nOne major advantage of the scaled residuals is that multiplying the observations by any positive number does not change the relative residuals. The relative residuals are scale free!\n\n<!-- Perhaps ask why we want sqrt variance not variance i.e. the units-->\n\n### Pearson correlation formula\n\nThe correlation is simply the covariance using the scaled residuals!\n\n\n$$\n\\text{cor}(x, y) = \\frac{\\frac{(x_1 - \\overline{x})}{sd(x)} \\frac{(y_1 - \\overline{y})}{sd(y)}  + \n\\frac{(x_2 - \\overline{x})}{sd(x)} \\frac{(y_2 - \\overline{y})}{sd(y)} + \\dots}{n} = \\frac{\\text{cov(x, y)}}{\\text{sd}(x) \\text{sd}(y)}\n$$\n\n\nIt is a mathematical fact that the pearson correlation is always between -1 and 1. Moreover it does not depend on the units/scale of the x or y data!\n\nNow let's compare the association between the child height and parent height using correlation. \n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfamily_heights %>% \n    summarise(father_cor = cov(father, childHeight),\n              mother_cor = cov(mother, childHeight),\n              mother_cor_femto = cor(mother_femtometers, childHeight))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  father_cor mother_cor mother_cor_femto\n       <dbl>      <dbl>            <dbl>\n1       2.29       1.64            0.201\n```\n:::\n:::\n\n\nNotice the change of units does not affect the correlation! \n\n\n# Moving beyond linearity\n\nNot all associations are linear. Each of the plots below shows to variables that are associated. The first column shows a moderate association; the second column shows a stronger association; the third column shows a perfect association (we call this a *deterministic relationship*).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup data\nn_samples = 200\n\nx_vals <- runif(n=n_samples, min=-1, max=1)\n\n\ny_lin_low <- x_vals + rnorm(n=n_samples, mean=0, sd=1)\ny_lin_med <- x_vals + rnorm(n=n_samples, mean=0, sd=.25)\ny_lin_det <- x_vals\n\ndeg <- 3\ny_mono_low <- x_vals^deg + rnorm(n=n_samples, mean=0, sd=1)\ny_mono_med <- x_vals^deg + rnorm(n=n_samples, mean=0, sd=.1)\ny_mono_det <- x_vals^deg\n\n\ny_nonlin_low <- cos(x_vals * (2 * pi )) + rnorm(n=n_samples, mean=0, sd=1)\ny_nonlin_med <- cos(x_vals * (2 * pi )) + rnorm(n=n_samples, mean=0, sd=.25)\ny_nonlin_det <- cos(x_vals * (2 * pi ))\n\ndf <- tibble(x_vals, \n             y_lin_low, y_lin_med, y_lin_det,\n             y_mono_low, y_mono_med, y_mono_det,\n             y_nonlin_low, y_nonlin_med, y_nonlin_det)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute correlations\nlibrary(XICOR)\n\n\nx_vals_vec <- df[, 1][[1]]\n\ncorr_df <- tibble(method = character(),\n                  variable = character(),\n                  corr = numeric())\nfor (method in c('pearson', 'spearman', 'chatterjee')){\n    for (j in 2:dim(df)[2]){\n        col <- colnames(df)[j]\n        \n        y_vals_vec <- df[, j][[1]]\n        \n        if (method == 'chatterjee'){\n            corr_val <- calculateXI(x=x_vals_vec, y=y_vals_vec)\n        }else{\n            corr_val <- cor(x_vals_vec, y_vals_vec, method=method)\n        }\n        \n        corr_df <- corr_df %>% add_row(method=method, variable=col, corr=corr_val)\n    }\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntitle_txt_size <- 5\nget_title <- function(corr_df, v){\n    \n    pc <- corr_df %>% filter(method=='pearson', variable==v) %>% .[[3]]\n    sc <- corr_df %>% filter(method=='spearman', variable==v) %>% .[[3]]\n    cc <- corr_df %>% filter(method=='chatterjee', variable==v) %>% .[[3]]\n    \n    \n    txt <- ''\n    txt <- paste0(txt, 'pearson = ', round(pc, 2))\n    txt <- paste0(txt, ', spearman = ', round(sc, 2))\n    txt <- paste0(txt, ', chatterjee = ', round(cc, 2))\n    txt\n}\n\nplot_lin_low <- df %>%\n    ggplot(aes(x=x_vals, y=y_lin_low)) +\n    geom_point() +\n    ggtitle(get_title(corr_df=corr_df, v='y_lin_low'))  + \n    theme(plot.title = element_text(size = title_txt_size))\n\n\nplot_lin_med <- df %>% \n    ggplot(aes(x=x_vals, y=y_lin_med)) +\n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_lin_med'))  + \n    theme(plot.title = element_text(size = title_txt_size))\n\nplot_lin_det <- df %>% \n    ggplot(aes(x=x_vals, y=y_lin_det)) + \n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_lin_det'))  + \n    theme(plot.title = element_text(size = title_txt_size))\n\nplot_mono_low <- df %>% \n    ggplot(aes(x=x_vals, y=y_mono_low)) + \n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_mono_low'))  + \n    theme(plot.title = element_text(size = title_txt_size))\n\nplot_mono_med <- df %>%\n    ggplot(aes(x=x_vals, y=y_mono_med)) + \n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_mono_med'))  + \n    theme(plot.title = element_text(size = title_txt_size))\n\nplot_mono_det <- df %>%\n    ggplot(aes(x=x_vals, y=y_mono_det)) + \n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_mono_det'))  + \n    theme(plot.title = element_text(size = title_txt_size))\n\nplot_nonlin_low <- df %>%\n    ggplot(aes(x=x_vals, y=y_nonlin_low)) +\n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_nonlin_low'))  + \n    theme(plot.title = element_text(size = title_txt_size))\n\nplot_nonlin_med <- df %>%\n    ggplot(aes(x=x_vals, y=y_nonlin_med)) + \n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_nonlin_med'))  + \n    theme(plot.title = element_text(size = title_txt_size))\n\nplot_nonlin_det <- df %>%\n    ggplot(aes(x=x_vals, y=y_nonlin_det)) +\n    geom_point() + \n    ggtitle(get_title(corr_df=corr_df, v='y_nonlin_det'))  + \n    theme(plot.title = element_text(size = title_txt_size))\n\n\n\nplot_grid(plot_lin_low, plot_lin_med, plot_lin_det,\n          plot_mono_low, plot_mono_med, plot_mono_det,\n          plot_nonlin_low, plot_nonlin_med, plot_nonlin_det,\n          ncol = 3, nrow = 3)\n```\n\n::: {.cell-output-display}\n![](notes-bivar_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThe first row shows a linear association. The second row shows a non-linear but *monotonic* association. Monotonic positive relation just means that if x goes up then y has to go up -- but it does not need to go up proportionally to the increase in x! A monotonic negative association just means the opposite (x goes up, y goes down). Monotonic is a generalization of linear (linear is monotonic, but montonic is not necessarily linear). We can quantify a monotonic relationship with [Spearman's rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient).\n\nThe third row shows a non-linear association that non-monotonic. Quantifying such general associations is not easy. In fact [statisticians are still inventing](https://arxiv.org/abs/1909.10140) ways of quantifying general relationships such as Chatterjee's correlation coefficient!\n",
    "supporting": [
      "notes-bivar_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}