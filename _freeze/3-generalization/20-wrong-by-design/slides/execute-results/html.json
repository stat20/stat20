{
  "hash": "29054ff0020bd7db9defdd35935436e5",
  "result": {
    "markdown": "---\ntitle: \"Wrong By Design\"\nformat:\n  revealjs:\n    author: \"STAT 20: Introduction to Probability and Statistics\"\n    height: 900\n    width: 1600\n    theme: ../../assets/slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: c\n    incremental: false\n    center: false\n    menu: false\n    highlight-style: github\n    progress: false\n    code-overflow: wrap\n    title-slide-attributes:\n      data-background-image: ../../assets/stat20-hex-bg.png\n      data-background-size: contain\nexecute: \n  echo: false\n---\n\n\n## Agenda\n\n- Concept Questions\n- Practice Problems\n\n\n# Concept Questions\n\n## \n\n![](images/ppk-as-ht.png)\n\n:::poll\nInstead of constructing a confidence interval to learn about the parameter, we could assert the value of a parameter and see whether it is consistent with the data using a hypothesis test. Say you are interested in testing whether there is a clear majority opinion of support or opposition to the project.\n\n\\\n\nWhat are the null and alternative hypotheses?\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_dab2808e\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">01</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n:::notes\nThe null is the p = .5 and the alternative is that p != .5.\n\nThis brings up a good discussion of one- and two-tailed tests. When students share their answers, you can draw a picture of a null distribution on the board with the observed statistic as a vertical line and consider how the p-value calculation would change depending on the alternative hypothesis.\n\nThis also brings up a discussion of the link between hypothesis tests and confidence intervals. Below the picture of the null distribution on the board, you could draw a bootstrap distribution centered on the vertical line of the observed statistic. In the HT setting, you consider the location of the statistic relative to the null distribution. In making a decision with a CI, you consider the location of the parameter relative to the bootstrap distribution (or more generally, the sampling distribution of the statistic).\n:::\n\n\n## {auto-animate=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(stat20data)\n\nppk <- ppk %>%\n  mutate(support_before = Q18_words %in% c(\"Somewhat support\", \n                                          \"Strongly support\",\n                                          \"Very strongly support\"))\n```\n:::\n\n\n## {auto-animate=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(stat20data)\n\nppk <- ppk %>%\n  mutate(support_before = Q18_words %in% c(\"Somewhat support\", \n                                          \"Strongly support\",\n                                          \"Very strongly support\"))\nobs_stat <- ppk %>%\n  specify(response = support_before,\n          success = \"TRUE\") %>%\n  calculate(stat = \"prop\")\n```\n:::\n\n\n## {auto-animate=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(stat20data)\n\nppk <- ppk %>%\n  mutate(support_before = Q18_words %in% c(\"Somewhat support\", \n                                          \"Strongly support\",\n                                          \"Very strongly support\"))\nobs_stat <- ppk %>%\n  specify(response = support_before,\n          success = \"TRUE\") %>%\n  calculate(stat = \"prop\")\nobs_stat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse: support_before (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 0.339\n```\n:::\n:::\n\n\n## {auto-animate=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull <- ppk %>%\n  specify(response = support_before,\n          success = \"TRUE\") %>%\n  hypothesize(null = \"point\", p = .5) %>%\n  generate(reps = 500, type = \"draw\") %>%\n  calculate(stat = \"prop\")\n```\n:::\n\n\n## {auto-animate=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull <- ppk %>%\n  specify(response = support_before,\n          success = \"TRUE\") %>%\n  hypothesize(null = \"point\", p = .5) %>%\n  generate(reps = 500, type = \"draw\") %>%\n  calculate(stat = \"prop\")\nnull\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse: support_before (factor)\nNull Hypothesis: point\n# A tibble: 500 × 2\n   replicate  stat\n   <fct>     <dbl>\n 1 1         0.481\n 2 2         0.503\n 3 3         0.493\n 4 4         0.481\n 5 5         0.5  \n 6 6         0.505\n 7 7         0.502\n 8 8         0.488\n 9 9         0.499\n10 10        0.473\n# … with 490 more rows\n```\n:::\n:::\n\n\n## {auto-animate=\"true\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnull <- ppk %>%\n  specify(response = support_before,\n          success = \"TRUE\") %>%\n  hypothesize(null = \"point\", p = .5) %>%\n  generate(reps = 500, type = \"draw\") %>%\n  calculate(stat = \"prop\")\nvisualize(null) +\n  shade_p_value(obs_stat, direction = \"both\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n##\n\n:::poll\nWhat would a Type I error be in this context?\n:::\n\n:::notes\nAnswer: Concluding that there is a clear majority when in fact there is an even split.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_c5e75919\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">01</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n##\n\n:::poll\nWhat would a Type II error be in this context?\n:::\n\n:::notes\nAnswer: Concluding that there is an even split when in fact there is a clear majority.\n\nAfter the previous question, students should get this quite easily.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_b87dd29c\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">01</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n\n<!-- ##  -->\n\n<!-- Imagine a scenario when your sample consists of the entire population. After setting up the hypotheses for your test, you ponder the chance that you'll make a statistical error. -->\n\n<!-- :::poll -->\n<!-- What is the probability that you'll make a Type I error using $\\alpha = .05$? -->\n<!-- ::: -->\n\n<!-- ```{r} -->\n<!-- countdown::countdown(1) -->\n<!-- ``` -->\n\n\n# Hypothesis Testing Review\n\n# One goal for today\n\n[Learn why we don't accept the null hypothesis.]{.bigadage}\n\n\n## What is it good for?\n\nHypothesis tests have been shown to be valuable contributors to science ([p < .05]{.fragment .highlight-green})\nbut are sometimes abused ([p < .05]{.fragment .highlight-green}).\n\n- Used to assess the degree to which data is consistent with a particular model.\n- The most widely used tool in statistical inference.\n\n\n## Step 1\n\n. . .\n\nLay out your model(s).\n\n$H_0$: null model, business as usual  \n$H_A$: alternative model, business not as usual\n\n- Hypotheses are statments about the TRUE STATE of the world and should involve\n*parameters*, not *statistics*.\n- Hypotheses should suggest a *test statistic* that has some bearing on the claim.\n- The nature of $H_A$ determines one- or two-sided tests; default to two.\n\n\n## Step 2\n\nSelect a test statistic that bears on the null hypothesis.\n\n. . .\n\n::::{.columns}\n:::{.column width=\"50%\"}\n- $\\bar{x}$\n- $\\hat{p}$\n- $m$\n- $r$\n- $b_1$\n:::\n:::{.column width=\"50%\"}\n- $\\bar{x}_1 - \\bar{x}_2$\n- $\\hat{p}_1 - \\hat{p}_2$\n- $m_1 - m_2$\n- $\\chi^2$\n- *The list goes on...*\n:::\n::::\n\n\n## Step 3\n\nConstruct the appropriate null distribution.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n1. Permutation (when `null = \"independence\"`)\n2. Simulation (when `null = \"point\"`)\n3. Normal Approximation\n\n\n## Step 4\n\n. . .\n\nCalculate a measure of consistency between the observed test statistic (the data) and the null distribution (i.e., a p-value).\n\n::::{.columns}\n:::{.column margin=\"50%\"}\n- If your observed test stat is in the tails\n  - low p-val\n  - data is inconsistent with null hypothesis\n  - \"reject null hypothesis\".\n:::\n\n:::{.column margin=\"50%\" .fragment}\n- If your observed test stat is in the body\n  - high p-val\n  - data is consistent with the null hypothesis\n  - \"fail to reject the null hypothesis\".\n:::\n::::\n\n. . .\n\n> What can go wrong?\n\n\n# Decision Errors\n\n\n##  {background-image=\"images/covid-dashboard.png\" background-size=\"contain\"}\n\n\n## Grammar of Graphics review\n\n. . .\n\n:::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/covid-barchart.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\nWhat geometries are in use in this graphic?\n:::\n::::\n\n\n## {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/covid-positivity.png){fig-align='center' width=100%}\n:::\n:::\n\n\n**A simplified model**\n\n[UHS tests a sample of the Cal community every week and monitors the positivity rate (proportion of tests that are positive).]{.fragment} [Assume this is a random sample of constant size and that the test is perfectly accurate.]{.fragment} [Let $p$ be the positivity rate.]{.fragment}\n\n. . .\n\n:::: columns\n::: {.column width=\"45%\"}\n[$H_0$]{.inversebox} $\\quad p = 3\\%$ \n\nThe incidence of COVID at Cal is at a manageable level.\n:::\n\n::: {.column width=\"10%\"}\n:::\n\n::: {.column width=\"45%\"}\n::: {.fragment}\n[$H_A$]{.inversebox}  $\\quad p > 3\\%$ \n\nThe incidence of COVID at Cal is at an elevated level.\n:::\n:::\n::::\n\n. . .\n\n> **Decision protocol**: if there is a big enough spike in a given week, shift classes to remote.\n\n\n## Decision Errors\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/decision-errors-table.jpg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::notes\nBuild this plot up slowly on the board. Recommended order:\n\n1. Null dist. If the null distribution is true, than we'd observe p-hats from the null distribution on the left.\n2. Vertical line that demarcates rejection region from do-not-reject region.\n3. If we use alpha = .05, what is the chance that we generate a p-hat in the reject region? .05\n4. Alt dist. What if instead the alt were true with this particular p (draw alt dist on right)?\n5. If we're generating p-hats from the alt distribution, what's the chance we'd generate one that looks like it's from the null (i.e. in the fail-to-reject region)?\n6. If we're generating p-hats from the alt distribution, what is the chance we'd notice i.e. that change that we'd reject the null? \n\nThis can lead into a discussion of what effects power:\n\nWhat would we have to do to this graphic to increase the area under the curve that represents power?\n\n1. increase alpha\n2. bring the distributions farther apart (increase effect size)\n3. decrease the spread of the distributions (increasing sample size is the approach they'll be familiar with. this is also where clever methods in survey sampling and experimental design come in: blocking and stratification)\n:::\n\n# Error Rates and Statistical Power\n\n## Decision Errors Rates\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/decision-error-rates.jpg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## What affects the error rates?\n\n- **Sample size, $n$**: with increasing $n$, the variability of the null distribution will decrease.\n\n- **Changing $\\alpha$**: decreasing $\\alpha$ will decrease type I error but increase type II error.\n\n- **Increasing _effect size_**: change data collection process to separate the distribution under $H_A$ and decrease type II error.\n    - Ex: If you're testing whether a pain medicine provides pain relief, only conduct the test if using a medicine that you expect to have cause a dramatic decrease in pain.\n\n\n## {}\n\n::: poll\nConsider a setting where the Cal UHS testing system observes a positivity rate of 3.5% in a one week interval, double the previous week.  Administration needs to decide whether or not to move to remote learning. Which error would be worse?\n\nA. Moving to remote instruction when in fact the true number of cases on campus is still low.\n\nB. Failing to move to remote instruction when in fact the true number of cases on campus is elevated.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_778d61ed\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:2em;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">01</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n\n## Statistical Power\n\n**Power** is the probability that you will reject the null hypothesis if it is in fact false.\n\n$$ P(\\textrm{reject } H_0 | H_0 \\textrm{ is false}) $$\n\n. . .\n\n:::: columns\n::: {.column width=\"50%\"}\n> The more power, the higher the probability of finding an effect.\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/PeopleBirding.jpeg){width=70%}\n:::\n:::\n\n:::\n::::\n\n\n::: footer\nhttps://upload.wikimedia.org/wikipedia/commons/6/66/PeopleBirding.JPG\n:::\n\n## {background-image=\"images/unmasking-the-mask.png\" background-size=\"contain\"}\n\n:::notes\nThis is an example of how low-power studies resulted in a failure to detect that masks were in fact helpful. The plot on the next page is pretty tricky to interpret, so only show it if you've skimmed the paper and are ready to talk through the plot.\n:::\n\n\n## {background-image=\"images/power-masks.png\" background-size=\"contain\"}\n\n\n# One goal for today\n\n[Learn why we don't accept the null hypothesis.]{.bigadage}\n",
    "supporting": [
      "slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}