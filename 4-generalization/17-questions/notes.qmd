---
title: "Questions and Data Scope"
subtitle: A framework for making generalizations with data.
date: "10/19/2022"
---

[[Discuss]{.btn .btn-primary}](https://edstem.org) [[Reading Questions]{.btn .btn-primary}](https://www.gradescope.com/courses/416233) [[PDF]{.btn .btn-primary}](notes.pdf)

Let's pause and return to the data science lifecycle that was introduced at the beginning of this course. 

![](images/DataScienceLifecycle.jpg){fig-align=center width="400"}

(Note that we have modified the earlier diagram a bit to highlight the main actions in the data science lifecycle.)

We are at the point in the course where we want to begin to draw conclusions about the world in answer to our questions. It might sound surprising, but in order to make generalizations, we must carefully consider the connection between the question and the data. The *scope* of the data help us determine whether we can extend our
findings beyond the data we have analyzed. But, first we revisit the stages of the data lifecycle in more detail. 

## The Stages of the Data Science Lifecycle

There are four basic stages in the lifecycle: asking a question, obtaining data, understanding the data, and understanding the world.
We have made these stages very broad on purpose. In our experience, the mechanics of the lifecycle change frequently as data become more complex and we continue to develop new methodologies to address these new data scenarios. Despite these changes, we have found that almost every data project follows the four steps in our lifecycle.  


### Ask a Question. 

Asking good questions lies at the heart of statistics and data science, and recognizing different kinds of questions guides us in our analyses. For example, "How have house prices changed over time?" is very different from the question "How will this new policy affect house prices?" There are four broad categories of questions: descriptive, predictive, generalization, and causal. These labels refer to the kind of data analysis needed to answer the question. You have seen two of these approaches to data analysis so far, and we will soon address generalizations and causal questions. 
After we introduce generalization (also known as statistical inference), we will revisit the ask-a-question stage and demonstrate how to match the question with the type of data analysis to be carried out. 

Narrowing down a broad question into one that can be answered with data is a key element of the first stage in the life cycle. It can involve consulting the people participating in a study, figuring out how to measure something, designing data collection protocols, and more. A clear and focused research question helps us determine the data we need, the patterns to look for, and how to interpret results. These considerations help us plan the data collection phase of the life cycle.

### Obtain Data. 

When data are expensive and hard to gather and when our aim is to generalize from the data to the world, we define precise protocols for collecting the data. Other times, data are cheap and easily accessed. This is especially true for online data sources. For example, Twitter lets people quickly download millions of data points.  In fact, when data are plentiful, we sometimes start an analysis by obtaining data, exploring it, and then developing a  research question. In both situations, most data have missing, unusual, or improperly coded values, and we need to check data quality. Typically, we also must manipulate measurements and create new variables before we can analyze the data more formally. That is, we may need to modify the structure of a data frame, and clean and transform data values to prepare for analysis. 

### Understand the Data. 

This stage might be the end of the lifecycle, if our goal is simply to report descriptive statistics or to explore the data. When our goal is  to explore the data, we look for interesting patterns, relationships, and distributions in the data. We also continue to look for problems with the data.  In our experience, this stage can be highly iterative. Understanding the data can lead us back to the earlier stages where we, say, find that we need to modify or redo our data cleaning, acquire more data to supplement our analysis, or refine our research question given the limitations of the data. The descriptive and exploratory analyses that we carry out in this stage may adequately answer our question, or, we may need to go on to the next stage in order to make generalizations beyond our data.

### Understand the World. 

Often our goals are purely descriptive or exploratory, and the analysis ends at the `Understand the Data' stage of the lifecycle. At other times, we aim to quantify how well the trends we find generalize beyond our data. We may want to use a model that we have fitted to our data to make inferences about the world or give predictions for future observations. To draw inferences from a sample to a population, we use statistical techniques like A/B testing and confidence intervals. And to make predictions for future observations, we create other kinds of interval estimates and use test/train splits of the data.


Data are at the heart of the life cycle, and the quality of the data collection process can significantly impact the validity and accuracy of the data, the strength of the conclusions we can draw from an analysis, and the decisions we make. Here, we describe a general approach to understand data collection, and use it to evaluate the usefulness of the data in addressing the question of interest. Ideally, we aim for data to be representative of the phenomenon that we are studying, whether that phenomenon is a population characteristic, a physical model, or some type of social behavior. Typically, our data do not contain complete information, yet we want to use the data to accurately describe a population, estimate a scientific quantity, infer the form of a relationship between variables, or predict future outcomes. In all of these situations, if our data are not representative of the object of study, then our conclusions can be limited, possibly misleading, or even wrong.


## Target Population, Access Frame, Sample

<!-- This needs an immediate example after that first sentence, which is very dense. -->

The initial step in the data life cycle is to express the question of interest in the context of the subject area and consider the connection between the question and the data collected to answer that question. It's good practice to do this before even thinking about the analysis or modeling steps because it may uncover a disconnect where the question of interest cannot be directly addressed with the available data. As part of making the connection between the data and the topic of investigation, we identify the target population, the means of accessing the population, instruments of measurement, and additional protocols used in the collection process. These concepts help us understand the scope of the data, and are useful whether we aim to gain knowledge about a population, scientific quantity, physical model, or social behavior.

**Target Population.** The target population consists of the collection of elements that you ultimately intend to describe and draw conclusions about. An element may be a person in a group of people, a voter in an election, a tweet from a collection of tweets, or a county in a state. We sometimes call an element a unit (of observation). You have seen with the penguin study that the unit does not need to be a person. But, it can also be something quite abstract. We will get to that notion in a moment.

**Access Frame.** The access frame is the collection of elements that are accessible to you for measurement. These are the units by which you can access the target. Ideally, the access frame and population are perfectly aligned; meaning they consist of the exact same elements. However, the units in an access frame may be only a subset of the target population; additionally, the frame may include units that don’t belong to the population. For example, to find out how a voter intends to vote in an election, you might call people on the phone. Someone you call may not be a voter; they are in your frame but not in the population. On the other hand, a voter who never answers a call from an unknown number can't be reached; they are in the population but not in your frame.

**Sample.** The sample is the subset of units taken from the access frame to measure, observe, and analyze. The sample gives you the data to analyze to make predictions or generalizations about the population of interest. So far in the class, every data frame that you have encountered has been a sample.

The contents of the access frame, as compared to the target, and the method used to select units from the frame to be in the sample are important considerations in determining whether or not the data are representative of the target population. If the access frame is not representative of the target population, then the data from the sample is most likely not representative either. And, if the units are sampled in a biased manner, problems with being representative also arise.

<!-- General representativeness is not needed; representativeness w.r.t. the variables of interest is. How / where to bring up this distinction? -->

Importantly, when the **granularity** of the target or access frame do not match that needed to answer the question, then we must realign our question and data scope. For example, when the question refers to measurements on individuals but the access frame provides averages of these measurements across groups of people, then we must either redefine our question or collect different data.

<!-- Do you think it'd be helpful to define "data scope"? -->

<!-- Granularity isn't specifically defined here. Is it narrowly the degree of aggregation or something broader? For example, in baseball data (form Lab 5), there is one data frame where the unit is a team in a season. In another data frame, the unit is a player across all seasons. Some of the variables in the Teams data set are simply aggregated up from the player, but not all (e.g. Wins). How does the term "granularity" feature into this example? -->

<!-- My challenge with the term "granularity" is that if it's narrowly the degree of aggregation, maybe we should just call it that since it's less ambiguous. If it's more broadly the unit of observation, then we should call it that (or some analogous term). If it's something different than those two, then it feels like we need a more precise definition. -->

You will also want to consider time and place in the data scope. For example, the effectiveness of a drug trial tested in one part of the world where a disease is raging might not compare as favorably with a trial in a different part of the world where background infection rates are lower. Additionally, data collected for the purpose of studying changes over time, like the average daily CO~2~ in the atmosphere, change with weather patterns so we need to be mindful of this as we examine the data. At other times, there might be spatial patterns in the data. For example, environmental health data that are reported for each census tract in the State of California can be examined with maps to look for spatial correlations in cities and rural areas.

If you didn’t collect the data yourself, you will want to consider who did and for what purpose. This is especially relevant today since more data is collected by others, often for administrative purposes, with a different goal in mind. Taking a hard look at *found data* and asking yourself whether and how these data might be used to address your question can save you from making a fruitless analysis or drawing inappropriate conclusions.

For each of the following examples, we begin with a general question, narrow it to one that can be answered with data, and in doing so, we identify the target population, access frame, sample, and the units within them. These concepts are represented by circles in a diagram for data scope, where the configuration of their overlap helps reveal key aspects the scope. Also in each example, we describe relevant temporal and spatial features.

### Example: Kids watching TV

The Early Childhood Longitudinal Study (ECLS) followed children who were in kindergarten in 1998 through to eighth grade. Data were collected from principals, teachers, parents, and children, and included student test scores, family demographics, school demographics, etc. 
When the children were in the eighth-grade in 2006, they were given a paper-and-pencil questionnaire that asked questions how they spent their time after school, among other things.

The ECLS was developed under the sponsorship of the U.S. Department of Education, and carried out by Westat and the Educational Testing Service (ETS).
Children were selected through a multistage process, where geographic areas were chosen; then, within the selected regions, elementary schools (both public and private) were chosen; and at each school a sample of kindergartners were invited to participate (actually, their parents were asked permission for their children to participate). All together 21,00 kindergartners across the US participated.

![](images/ECLS-scope.jpg){fig-align=center width="400"}

The target population in this example is all eighth graders in the US in 2006. The access frame includes all of these children, except those who are home schooled, didn't attend kindergarten, or immigrated to the US after kindergarten. (The students had to have been available for selection as kindergartners.) While the method for choosing children for the sample is complex, we can identify the sample as a subset of those children in the access frame. We will soon cover different methods for choosing a sample. 

As for the question, we are interested in uncovering any connections between the amount a child watches TV after school and other factors, particularly those related to parental demographics. This study is an example of a large survey that aims to provide both descriptive data on childrens' status as they begin school and 
their progress into middle school, and to provide a rich data set that enables researchers to analyze a wide range of family, school, community, and individual variables that may be related to children's early success in school.

### Example: Environmental Hazards and Health

How do environmental hazards impact an individual's health in California? To address this question, the California Environmental Protection Agency developed the CalEnviroScreen project. This project studies connections between population health and environmental pollution in California communities. It uses data collected from several sources that includes demographic summaries from the U.S. Census, health statistics from the California Office of Statewide Health Planning and Development, and pollution measurements from air monitoring stations around the state.

Ideally, we want to study the individuals living in California and assess the impact of these environmental hazards on their health. That is, our target population is all people in the state of California, and the unit is one person. However, the granularity of the available data prohibit access to individual measurements because the data are reported for census tracts. 

![](images/CA-enviro-scope.jpg){fig-align=center width="400"}

The access frame consists of groups of residents living in the same census tract. This means that a unit in the access frame is a census tract. Since measurements for all of the census tracts are available, the sample is a census. That is, all census tracts in California are in the sample.

Unfortunately, we cannot dis-aggregate the information in a tract to examine what happens to an individual person. This aggregation impacts the question we can address and the conclusions that we can draw. We need to refine our question to ask, instead, about the environmental factors in a census tract that are related to health indicators for the region.


### Example: Flights out of the Bay Area

We are interested in discovering factors that are related to flight delays. One approach is to describe our population of interest as all flights landing or taking off around the world. However, the data that we have available are all flights out of San Francisco and Oakland in 2020. If we think that flights out of the Bay Area might be different in important ways from those in other locations (which makes sense, right?), then we must narrow the focus of the question to Bay Area flights.

So, the unit of observation is a flight that departs from SFO or OAK in 2020, and the population are all of these flights. This is a situation where administrative data have been collected by a federal agency and made available to the public. In this case, the access frame aligns with the population, and the sample consists of the entire access frame. 

If the flight data consisted of commercial passenger, private, and cargo flights, then the access frame would include flights that are outside of our population, and we would want to trim those units from our sample. Below is a diagram for this scenario.

![](images/Flights-scope.jpg){fig-align=center width="400"}


### Example: COVID Vaccine Trials

In 2020, Johnson & Johnson carried out a clinical trial for their new COVID vaccine. They wanted to determine the efficacy of their vaccine, i.e., how much more were people with the vaccine able to  resist infection from COVID-19 compared to those who did not receive any vaccine. 

To answer this question, they enrolled 43,738 people in the trial. These volunteers were:  

+ adults 18 and over;
+ participants in the study from October to November, 2020;
+ from 8 countries across 3 continents, including the US and South Africa.

The participants were split into two groups at random. Half received the new vaccine, and the other half received a placebo, a fake vaccine such as a saline solution. Then, everyone was followed for 28 days to see whether they contracted COVID-19.

![](images/COVID-scope.jpg){fig-align=center width="400"}

The target population consists of all adults in the world at the time of the trial, but the access frame consists of individuals willing to volunteer for the trial living in the 8 countries where the trial took place.  We don't know exactly how the sample of volunteers was taken, but an effort was made to include many people (40%) who have conditions that might cause serious complications from COVID infection. 

This is an example where the time and location of the study are important to include in the scope. The disease went in waves across the world with people in some countries being hardest at different times.


## Instruments and Protocols

When we consider the scope of the data, we also consider the instrument being used to take the measurements and the procedure for taking measurements, which are a part of what we call the protocol. For a survey, the instrument is typically a questionnaire that an individual in the sample answers. The protocol for a survey includes how the sample is chosen, how nonrespondents are followed up, interviewer training, protections for confidentiality, etc.

Good instruments and protocols are important to all kinds of data collection. If we want to measure a natural phenomenon, such as the length of the orbit of an asteroid that has been hit by a spacecraft, we need to quantify the accuracy of the instrument. The protocol for calibrating the instrument and taking measurements is vital to obtaining accurate measurements. Instruments can go out of alignment and measurements can drift over time leading to poor, highly inaccurate measurements.

Protocols are also critical in experiments. Ideally, any factor that can influence the outcome of the experiment is controlled. For example, temperature, time of day, confidentiality of a medical record, and even the order of taking measurements need to be consistent to rule out potential effects from these factors getting in the way.

Many data analysis projects involve linking data together from multiple sources. Each source needs to be examined through this data-scope construct and any difference across sources considered. Additionally, matching algorithms used to combine data from multiple sources need to be clearly understood so that populations and frames from the sources can be compared.

The scope-diagram introduced for observing a target population can be extended to the situation where we want to measure a fixed quantity such as the age of a fossil, the length of an asteroid's orbit, etc. In these cases we consider the quantity we want to measure as an unknown value. (This unknown value referred is often referred to as a *parameter*.) In our diagram, we shrink the target to a point that represents this unknown. The instrument’s accuracy acts as the frame, and the sample consists of the measurements taken by the instrument within the frame. You might think of the frame as a dart board, where the instrument is the person throwing the darts. If they are reasonably good, the darts land within the circle, scattered around the bulls‐ eye. The scatter of darts correspond to the measurements taken by the instrument. The target is not seen by the dart thrower, but ideally it coincides with the bulls‐eye.

### Example: DART spacecraft

Recently, a NASA spacecraft, part of the DART project, traveled for ten months to purposefully hit an asteroid, Dimorphos, which orbits around a larger asteroid, Didymos. The goal of this mission was to study the possibility of using a spacecraft to impact the path of an asteroid. After the collision, astronomers used telescopes to measure how much the orbit changed. They found that the spacecraft altered Dimorphos' orbit by 32 minutes, shortening the orbit to 11 hours and 23 minutes.

Before and after the impact of the spacecraft, telescopes measured the brightness of Dimorphos as it crossed in front and behind Didymos during its orbit. The dips in these measurements produce light curves that show the periodic changes in brightness over time which gives the length of the orbit. By comparing the light curves from before and after the impact, astronomers estimated the change in length of the asteroid's orbit. 

Crucial to this process is the accuracy of the telescope measurements of the light reflected by the asteroid. The telescopes are finely tuned instruments that make precise measurements, which have allowed scientists to attach a margin of uncertainty of 2 minutes to the estimate of the orbit length. The top left portion of the diagram in the next section represents the data scope. There, the true length of the orbit is represented by the star, which we call a parameter. The access frame can be equated to the telescope, or the circle that contains the star. And the sample is those measurements from the telescope that are marked as dots around the star. 


## Accuracy

When we have a census, the sample captures the entire population (and the access frame matches the population). In this situation, if we administer a well-designed questionnaire, then we have complete and accurate knowledge of the population, and the scope is complete. Similarly in measuring the length of an asteroid's orbit, if our instrument has perfect accuracy and is properly used, then we can measure the exact length of the orbit.

These situations are rare, if not impossible. In most settings, we need to quantify the accuracy of our measurements in order to generalize our findings to the unobserved. For example, we often use the sample to estimate an average value for a population, infer the value of a scientific unknown, or predict the behavior of a new individual. In each of these settings, we also want to quantify the accuracy of our estimates and predictions. That is, we want to know how close our estimates, inferences, and predictions are to the truth.

The analogy of darts thrown at a dart board can be useful in understanding accuracy. We divide accuracy into two basic parts: bias and variance (also known as precision). Our goal is for the darts to hit the bullseye on the dart board and for the bullseye to line up with the unseen target. The spray of the darts on the board represents the variance in our measurements, and the gap from the bullseye to the unknown value that we are targeting represents the bias. 

![](images/Bias-Variance-Bullseye.jpg){fig-align=center width="400"}

Representative data puts us in the top row of the diagram, where there is low bias, meaning that the bullseye and the unseen target are in alignment. Ideally our instruments and protocols put us in the upper left part of the diagram, where the variance is also low. The pattern of points in the bottom row systematically miss the targeted value. Taking larger samples will not correct this bias.

### Types of Bias

Bias comes in many forms. We describe some classic types here and connect them to our target-access-sample framework.

+ Coverage bias occurs when the access frame does not include every unit in the target population. For example, a survey based on phone calls cannot reach those without phones. In this situation, those who cannot be reached may differ in important ways from those in the access frame.

+ Selection bias arises when the mechanism used to choose units for the sample tends to select certain units more often than they should. As an example, a convenience sample chooses the units most easily available. Problems can arise when those who are easy to reach differ in important ways from those harder to reach. Another example of selection bias can happen with observational studies and experiments. These studies often rely on volunteers (people who choose to participate), and this self-selection has the potential for bias if the volunteers differ from the target population in important ways.

+ Non-response bias comes in two forms: unit and item. Unit non-response happens when someone selected to be in the sample is unwilling to participate. Item non-response occurs when, say, someone in the sample refuses to answer a particular survey question. Non-response can lead to bias if those who choose to not participate or to not answer a particular question are systematically different from those who respond.

+ Measurement bias happens when an instrument systematically misses the target in one direction. For example, low humidity can systematically give us incorrectly high measurements for air pollution. In addition, measurement devices can become unstable and drift over time and so produce systematic errors. In surveys, measurement bias can arise when questions are confusingly worded or leading, or when respondents may not be comfortable answering honestly.

Each of these types of bias can lead to situations where the data are not centered on the unknown targeted value. Often we are not able to assess the potential magnitude of the bias, since little to no information is available on those who are outside of the access frame, less likely to be selected for the sample, or disinclined to respond. Protocols are key to reducing these sources of bias. Chance mechanisms to select a sample from the frame or to assign units to experimental conditions can eliminate selection bias. A non-response follow-up protocol to encourage participation can reduce non-response bias. A pilot survey can improve question wording and so reduce measurement bias. Procedures to calibrate instruments and protocols to take measurements in, say, random order can reduce measurement bias.

Bias does not need to be avoided under all circumstances. If an instrument is highly precise (low variance) and has a small bias, then that instrument might be preferable to another that has high variance and little to no bias. Biased studies can be useful to pilot a survey instrument or to capture useful information for the design of a larger study. Many times we can at best recruit volunteers for a study. Given this limitation, it can still be useful to enroll these volunteers in the study and use random assignment to split them into treatment groups. That’s the idea behind randomized controlled experiments.

Whether or not bias is present, data typically also exhibit variation. Variation can be introduced purposely by using chance to select a sample, and it can occur from an instrument’s precision or natural variation.


### Types of Variation

Variation that results from a chance mechanism has the advantage of being quantifiable.

+ Sampling variation results from using chance to take a sample. We can in principle compute the chance a particular sample is selected.

+ Assignment variation of units to treatment groups in a controlled experiment produces variation. If we split the units up differently, then we can get different results from the experiment. This randomness allows us to compute the chance of a particular group assignment.

+ Measurement error for instruments result from a measurement process. If the instrument has no drift and a reliable distribution of errors, then when we take multiple measurements on the same object, we get variations in measurements that are centered on the truth.

The Box Model is a simple abstraction that can be helpful for understanding variation. This model helps us reason about sampling schemes, randomized controlled experiments, and measurement error. It is the topic of the next section.

## Summary

No matter the kind of data you are working with, before diving into cleaning, exploration, and analysis, take a moment to look into the data source. If you didn’t collect the data, ask yourself:

+ Who collected the data?
+ Why were the data collected?

Answers to these questions can help determine whether these found data can be used to address the question of interest to you.

Consider the scope of the data. Questions about the temporal and spatial aspects of data collection can provide valuable insights:

+ When were the data collected?
+ Where were the data collected?

Answers to these questions help you determine whether your findings are relevant to the situation that interests you or are possibly not comparable to your place and time.

Core to the notion of scope are answers to the following questions:

+ What is the target population (or unknown parameter value)?
+ How is the target accessed?
+ Are the units in the target the same as the units in the access frame, and are they at the level of granularity needed to address the question?
+ What methods are used to select samples/take measurements?
+ What instruments are used and how are they calibrated?
+ Do the measurements made on the units provide the information needed to answer the question?

Answering as many of these questions as possible can give you valuable insights as to how much trust you can place in your findings and how far you can generalize them.

These notes have provided you with a terminology and framework for thinking about and answering these questions. We have also outlined ways to identify possible sources of bias and variance that can impact the accuracy of your findings.  Next, we develop the Box model for situations when a chance mechanism can be used to select a sample from an access frame, divide volunteers into experimental treatment groups, or take measurements from a well-calibrated instrument.


